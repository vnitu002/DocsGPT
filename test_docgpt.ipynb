{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"application/prompts/history_questions.txt\", \"r\") as f:\n",
    "    history_questions = f.readlines()\n",
    "\n",
    "with open(\"application/prompts/history_answers.txt\", \"r\") as f:\n",
    "    history_answers = f.readlines()\n",
    "\n",
    "hist = []\n",
    "for i in range(len(history_questions)):\n",
    "    hist.append({\"prompt\": history_questions[i].strip(), \"response\": history_answers[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'How do I get started?',\n",
       "  'response': '# Install integrate.ai SDK and Client\\\\n\\\\n## Generate an access token\\\\n\\\\nTo install the client and SDK, you must generate an access token through the web portal. \\\\n1. Log in to your workspace through the portal.\\\\n2. On the Dashboard, click Generate Access Token.\\\\n3. Copy the acccess token and save it to a secure location.\\\\n\\\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\\\n\\\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as `<IAI_TOKEN>`.\\\\n\\\\n## Download the sample notebook\\\\n\\\\n\\\\n## Install integrate.ai packages\\\\n\\\\n1. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: \\\\n`pip install integrate-ai`\\\\n2. Install the SDK package using the access token you generated.\\\\n`iai sdk install --token <IAI_TOKEN>`\\\\n3. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container.\\\\n`iai client pull --token <IAI_TOKEN>`\\\\n\\\\n*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment.\\n'},\n",
       " {'prompt': 'How do I use a custom model?',\n",
       "  'response': 'Using the template files provided, create a custom model package. \\\\n\\\\nFollow the naming convention for files in the custom package: no spaces, no special characters, no hyphens, all lowercase characters.\\\\n\\\\n1. Create a folder to contain your custom model package. For this tutorial, this folder is named myCustomModel, and is located in the same parent folder as the template folder.\\\\n\\\\nExample path: C:\\\\<workspace>\\\\integrate_ai_sdk\\\\sample_packages\\\\myCustomModel\\\\n\\\\n2. Create two files in the custom model package folder: \\\\n    a. `model.py` - the custom model definition. You can rename the template_model.py as a starting point for this file.\\\\n    b. `<model-class-name>.json` - default model inputs for this model. It must have the same name as the model class name that is defined in the model.py file. \\\\n\\\\nIf you are using the template files, the default name is `templatemodel.json`.\\\\n\\\\n3. _Optional_: To use a custom dataloader, you must also create a `dataset.py` and a dataset configuration JSON file in the same folder. For more information, see [#](Creating a Custom Dataloader).\\\\n\\\\nIf there is no custom dataset file, the default `TabularDataset` loader is used. It loads .parquet  and .csv files, and requires predictors: [\"x1\", \"x2\"], target: y as input for the data configuration. This is what is used for the standard models.\\\\n\\\\nThe example below provides the boilerplate for your custom model definition. Fill in the code required to define your model. Refer to the model.py files provided for the lstmTagger and cifar10_vgg16 examples if needed. \\\\n\\\\n```python\\\\nfrom integrate_ai_sdk.base_class import IaiBaseModule\\\\n\\\\nclass TemplateModel(IaiBaseModule):\\\\n    def __init__(self):\\\\n        \"\"\"\\\\n        Here you should instantiate your model layers based on the configs.\\\\n        \"\"\"\\\\n        super(TemplateModel, self).__init__()\\\\n\\\\n    def forward(self):\\\\n        \"\"\"\\\\n        The forward path of a model. Can take an input tensor and return a prediction tensor\\\\n        \"\"\"\\\\n        pass\\\\n\\\\nif __name__ == \"__main__\":\\\\n    template_model = TemplateModel()\\\\n```\\n'},\n",
       " {'prompt': 'How do I generate a non-admin token?',\n",
       "  'response': 'Create a scoped token for the user. Include only the scopes that the user requires to work with the system and their data. \\\\n\\\\n```python\\\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\\\nprint(token)p\\\\n```\\\\n\\\\nThis request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \\\\n\\\\nCopy and save the token somewhere secure to share with the user. '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query(question, url=\"http://0.0.0.0:7091/api/answer\", history=hist):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"history\": history,\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"embeddings_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    "\n",
    "    res = requests.post(url=url, data=json.dumps(payload), headers=headers)\n",
    "    display(Markdown(res.json()[\"answer\"]))\n",
    "    print(\"-------------------------------------\")\n",
    "    for doc in res.json()[\"question_sources\"]:\n",
    "         print(doc)\n",
    "    print(\"-------------------------------------\")\n",
    "    for doc in res.json()[\"answer_sources\"]:\n",
    "        print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAI DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To get started, you will need to install the integrate.ai SDK and Client. \n",
       "\n",
       "1. Generate an access token through the web portal. Log in to your workspace through the portal, and on the Dashboard, click Generate Access Token. Copy the acccess token and save it to a secure location. \n",
       "\n",
       "2. Download the sample notebook. \n",
       "\n",
       "3. Install the integrate.ai packages. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: `pip install integrate-ai`. Install the SDK package using the access token you generated. `iai sdk install --token <IAI_TOKEN>`. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container. `iai client pull --token <IAI_TOKEN>`. \n",
       "\n",
       "*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.45616212', 'text': '\\n\\nDownload the sample notebook\\n\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.46205103', 'text': '\\n\\nRequirements\\n\\nThis section outlines the setup steps required to configure your working environment. Steps that are performed in the AWS platform are not explained in detail. Refer to the AWS documentation as needed. \\n\\nThe requirements are tool-agnostic - that is, you can complete the steps through the AWS console, or through a tool such as Terraform or AWS CloudFormation. \\n\\n', 'title': 'input/aws-batch-manual.md'}\n",
      "{'score': '0.4785251', 'text': '\\n\\nWindows Setup\\n\\n1. Ensure that intel VT-x or AMD SVM is enabled in BIOS, check the motherboard manufacture document for exact steps.\\n2. Install CUDA driver or CUDA toolkit:\\n* Install cuda toolkit (which include driver, but also contains other unnecessary components)\\n** In the component selection screen, you can choose to install only the CUDA driver\\n* Install CUDA driver only\\n** Go to Nvidia driver page, select the corresponding graphic card and operating system, download and install the driver.\\n3. Setup Docker with WSL2.\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.08648345', 'text': '\\n\\nInstall integrate.ai packages\\n\\n1. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: \\n`pip install integrate-ai`\\n2. Install the SDK package using the access token you generated.\\n`iai sdk install --token `\\n3. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container.\\n`iai client pull --token `\\n\\n*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment.\\n\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.17781419', 'text': '\\n\\nInstall integrate.ai SDK and Client\\n\\n \\n\\nGenerate an access token\\n\\nTo install the client and SDK, you must generate an access token through the web portal. \\n1. Log in to your workspace through the portal.\\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the acccess token and save it to a secure location.\\n\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as ``.\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.2229155', 'text': '\\n\\nFor Developers\\n\\nTo run the integrate.ai SDK samples and build models, ensure that your environment is configured properly. \\n\\nRequired software:\\n* Python 3.8 (or later)\\n* Pip 22.2 (or later)\\n* Docker\\n\\nTo run the sample notebook, you may also need:\\n* Jupyter notebook (or other code tool such as VS Code)\\n\\n', 'title': 'input/install-sdk.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"How do I get started?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Yes, I have sample code for a variety of tasks. For example, here is sample code for creating a scoped token for a user: \n",
       "\n",
       "```python\n",
       "token = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\n",
       "print(token)p\n",
       "```\n",
       "\n",
       "This request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \n",
       "\n",
       "Copy and save the token somewhere secure to share with the user."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3772081', 'text': '\\n\\nDownload the sample notebook\\n\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.44656748', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "{'score': '0.46398148', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.14310782', 'text': '\\n\\nCreate a scoped user token\\n\\nCreate a scoped token for the user. Include only the scopes that the user requires to work with the system and their data. \\n\\n```python\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\nprint(token)p\\n```\\n\\nThis request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \\n\\nCopy and save the token somewhere secure to share with the user. \\n\\n**Available Scopes**\\n\\n\\n\\n\\n', 'title': 'input/user-auth.md'}\n",
      "{'score': '0.17657867', 'text': '\\n\\nCreate user and create scoped API token\\n\\n\\n```python\\nuser_name = \"demo-user\"\\nuser = auth_client.create_user(user_name)\\n```\\n\\n\\n```python\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\nprint(token)\\n```\\n\\n\\n```python\\nauth_client.token_info(token[\"token\"])\\n```\\n\\n', 'title': 'input/integrateai_auth.md'}\n",
      "{'score': '0.3008583', 'text': '\\n\\nManaging User Access through the SDK\\n\\nCreate a connection to the auth client with your unscoped token.\\n\\n```python\\nfrom integrate_ai_sdk.auth import connect_to_auth_client\\nfrom integrate_ai_sdk.auth.scopes import Scope\\nimport os\\nIAI_TOKEN = os.environ.get(\"IAI_TOKEN\")\\nauth_client = connect_to_auth_client(token=IAI_TOKEN)\\n```\\n\\n', 'title': 'input/user-auth.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"Do you have sample code?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To use a custom model, you must first create a custom model package. Follow the naming convention for files in the custom package: no spaces, no special characters, no hyphens, all lowercase characters.\n",
       "\n",
       "1. Create a folder to contain your custom model package. For this tutorial, this folder is named myCustomModel, and is located in the same parent folder as the template folder.\n",
       "\n",
       "Example path: C:\\<workspace>\\integrate_ai_sdk\\sample_packages\\myCustomModel\n",
       "\n",
       "2. Create two files in the custom model package folder: \n",
       "    a. `model.py` - the custom model definition. You can rename the template_model.py as a starting point for this file.\n",
       "    b. `<model-class-name>.json` - default model inputs for this model. It must have the same name as the model class name that is defined in the model.py file. \n",
       "\n",
       "If you are using the template files, the default name is `templatemodel.json`.\n",
       "\n",
       "3. _Optional_: To use a custom dataloader, you must also create"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.37047327', 'text': '\\n\\nCustom model, dataset, and LSTMTagger.json\\nChoose a name for your custom model, and set the path for the model and data configurations.  \\nNote that the name for your custom model **must be unique**.  \\nThis means that the name for your custom model cannot already be in the Package Name column of the Custom Models Packages Table in the Model Library Page of the UI.\\n\\n\\n\\n```python\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.4369074', 'text': '\\n\\nLoad the saved model\\n\\nTo load a model saved previously, a model object needs to be initialized first. This can be done by directly importing one of the IAI-supported packages (e.g., FFNet) or using the model class defined in a custom package. \\n\\n\\n```python\\nfrom integrate_ai_sdk.sample_packages.lstmTagger.model import LSTMTagger\\n\\nmodel = LSTMTagger(embedding_dim=4, hidden_dim=3, output_size=4, vocab_size=9)\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.4469043', 'text': '\\n\\nLoad the saved model\\n\\nTo load a model saved previously, a model object needs to be initialized first. This can be done by directly importing one of the IAI-supported packages (e.g., FFNet) or using the model class defined in a custom package. \\n\\n\\n```python\\nfrom integrate_ai_sdk.packages.FFNet.nn_model import FFNet\\n\\nmodel = FFNet(input_size=15, output_size=2, hidden_layer_sizes=[6, 6, 6])\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.30767328', 'text': '\\n\\nCustom model, dataset, and LSTMTagger.json\\nChoose a name for your custom model, and set the path for the model and data configurations.  \\nNote that the name for your custom model **must be unique**.  \\nThis means that the name for your custom model cannot already be in the Package Name column of the Custom Models Packages Table in the Model Library Page of the UI.\\n\\n\\n\\n```python\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.39322677', 'text': '\\n\\nUpdate the following lines to your package name and model and data configuration paths\\npackage_name = \"lstm_sample_package\"\\nmodel_config_path = \"../lstmTagger/lstmtagger.json\"\\ndata_config_path = \"../lstmTagger/taggerDataset.json\"\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.40133995', 'text': '\\n\\nUpload customized model\\n\\n\\n\\n```python\\nclient.upload_model(\\n    package_path=package_path,\\n    dataset_path=dataset_path,\\n    package_name=package_name,\\n    sample_model_config_path=model_config_path,\\n    sample_data_config_path=data_config_path,\\n    batch_size=256,\\n    task=\"classification\",\\n    test_only=False,\\n    description=\"A custom LSTM model.\",\\n)\\n```\\n\\n \\n\\nCreate a Session\\n\\nThe Quickstart guide for creating a session gives a bit more context into the paramters that are used during session creation.\\nFor this session we are going to be using two training clients and two rounds. \\n\\n\\n```python\\nmodel_config = {\\n    \"strategy\": {\"name\": \"FedAvg\", \"params\": {}},\\n    \"model\": {\"params\": model_config},\\n    \"ml_task\": {\"type\": \"classification\", \"params\": {}},\\n    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.9, \"momentum\": 0.9}},\\n    \"differential_privacy_params\": {\"epsilon\": 4, \"max_grad_norm\": 7},\\n    \"seed\": 23,  # for reproducibility\\n}\\n```\\n\\n\\n```python\\nsession = client.create_fl_session(\\n    name=\"LSTM custom model notebook\",\\n    description=\"Training a custom LSTM model using a jupyter notebook.\",\\n    min_num_clients=2,\\n    num_rounds=5,\\n    package_name=package_name,\\n    model_config=model_config,\\n    data_config=data_schema,\\n).start()\\n\\nsession.id\\n```\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"How do I use a custom model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Create a scoped token for the user. Include only the scopes that the user requires to work with the system and their data. \n",
       "\n",
       "```python\n",
       "token = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\n",
       "print(token)p\n",
       "```\n",
       "\n",
       "This request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \n",
       "\n",
       "Copy and save the token somewhere secure to share with the user."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.43495193', 'text': '\\n\\nVerify user and token through the UI\\n\\nTo confirm that the user and token were created successfully, you can also view them in the web dashboard. \\n\\n1. Log in to the web dashboard.\\n2. Click Token Management.\\n2. Click User Scoped Tokens.\\n3. Locate the user name for the user you created. \\n\\n', 'title': 'input/user-auth.md'}\n",
      "{'score': '0.45202243', 'text': \"\\n\\nCustom Models\\n\\n\\n\\n\\n \\n\\nUser Authentication\\n\\nSharing access to training sessions and shared models in a simple and secure manner is a key requirement for many data custodians. integrate.ai provides a secure method of authenticating end users with limited permissions through the SDK to enable privileged access. \\n\\nAs the user responsible for managing access through the integrate.ai platform, you have the ability to generate an **unscoped API token** through the integrate.ai UI. Unscoped API tokens provide full access to the integrate.ai SDK. You can run client training tasks locally, or on remote data. \\n\\nIn the case that you want to create a token that has limited access, to enforce governance standards or provide an end user of your platform with limited access to the integrate.ai SDK, you can create **scoped API tokens**. Scoped tokens grant limited permissions, which enables you to control the level of access to trained sessions and models.\\n\\nIn the UI, you can view your personal API tokens as well as all scoped API tokens created in your organization's workspace through the SDK. These scoped user tokens are designed for use with the integrate.ai SDK. Tokens are tied to user identities through a unique ID, which is logged with each user action.\\n\\nLimiting user access by token greatly reduces the security risk of leaked credentials. For example, with an unscoped API token, a user could run tasks on a remote machine, where there is a risk that it could be leaked or exposed because it is shared in an outside (non-local) environment. To mitigate that risk, you can instead provide the user with a scoped token that has limited permissions and a short lifespan (maximum 30 days).\\n\\n \\n\\nCreate an unscoped token\\n\\nAs the user who manages other users' acce\\nss, you must first create your own unscoped token.\\n\\n1. Log in to your integrate.ai account on the web. \\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the access token and save it to a secure location. \\n\\nThis is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as .\\n\\n\", 'title': 'input/train-overview.md'}\n",
      "{'score': '0.47355956', 'text': '\\n\\nInstall integrate.ai SDK and Client\\n\\n \\n\\nGenerate an access token\\n\\nTo install the client and SDK, you must generate an access token through the web portal. \\n1. Log in to your workspace through the portal.\\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the acccess token and save it to a secure location.\\n\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as ``.\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.038432084', 'text': '\\n\\nCreate a scoped user token\\n\\nCreate a scoped token for the user. Include only the scopes that the user requires to work with the system and their data. \\n\\n```python\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\nprint(token)p\\n```\\n\\nThis request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \\n\\nCopy and save the token somewhere secure to share with the user. \\n\\n**Available Scopes**\\n\\n\\n\\n\\n', 'title': 'input/user-auth.md'}\n",
      "{'score': '0.1504609', 'text': '\\n\\nCreate user and create scoped API token\\n\\n\\n```python\\nuser_name = \"demo-user\"\\nuser = auth_client.create_user(user_name)\\n```\\n\\n\\n```python\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\nprint(token)\\n```\\n\\n\\n```python\\nauth_client.token_info(token[\"token\"])\\n```\\n\\n', 'title': 'input/integrateai_auth.md'}\n",
      "{'score': '0.31761742', 'text': '\\n\\nManaging User Access through the SDK\\n\\nCreate a connection to the auth client with your unscoped token.\\n\\n```python\\nfrom integrate_ai_sdk.auth import connect_to_auth_client\\nfrom integrate_ai_sdk.auth.scopes import Scope\\nimport os\\nIAI_TOKEN = os.environ.get(\"IAI_TOKEN\")\\nauth_client = connect_to_auth_client(token=IAI_TOKEN)\\n```\\n\\n', 'title': 'input/user-auth.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"how do I generate a non-admin token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To renew your token, create a new API token and revoke the old one. You can manage API tokens through the web portal. \n",
       "\n",
       "1. Log in to your workspace through the portal.\n",
       "2. On the Dashboard, click Generate Access Token.\n",
       "3. Copy the acccess token and save it to a secure location.\n",
       "4. Revoke the old token.\n",
       "5. Use the new token for all future requests."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3911299', 'text': '\\n\\nVerify user and token through the UI\\n\\nTo confirm that the user and token were created successfully, you can also view them in the web dashboard. \\n\\n1. Log in to the web dashboard.\\n2. Click Token Management.\\n2. Click User Scoped Tokens.\\n3. Locate the user name for the user you created. \\n\\n', 'title': 'input/user-auth.md'}\n",
      "{'score': '0.46478128', 'text': \"\\n\\nRevoke a scoped token\\n\\nUser scoped tokens have a default lifespan of thirty (30) days. To revoke a token before it expires, use the revoke_token command in the SDK. \\n\\nYou must provide the `token_id` for the token that you want to revoke. You can find this ID in the web dashboard.\\n\\n```python\\nauth_client.revoke_token(token['token_id'])\\n```\\n\\n\", 'title': 'input/user-auth.md'}\n",
      "{'score': '0.47211307', 'text': '\\n\\nInstall integrate.ai SDK and Client\\n\\n \\n\\nGenerate an access token\\n\\nTo install the client and SDK, you must generate an access token through the web portal. \\n1. Log in to your workspace through the portal.\\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the acccess token and save it to a secure location.\\n\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as ``.\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.22749096', 'text': '\\n\\nInstall integrate.ai SDK and Client\\n\\n \\n\\nGenerate an access token\\n\\nTo install the client and SDK, you must generate an access token through the web portal. \\n1. Log in to your workspace through the portal.\\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the acccess token and save it to a secure location.\\n\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as ``.\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.34906328', 'text': \"\\n\\nCustom Models\\n\\n\\n\\n\\n \\n\\nUser Authentication\\n\\nSharing access to training sessions and shared models in a simple and secure manner is a key requirement for many data custodians. integrate.ai provides a secure method of authenticating end users with limited permissions through the SDK to enable privileged access. \\n\\nAs the user responsible for managing access through the integrate.ai platform, you have the ability to generate an **unscoped API token** through the integrate.ai UI. Unscoped API tokens provide full access to the integrate.ai SDK. You can run client training tasks locally, or on remote data. \\n\\nIn the case that you want to create a token that has limited access, to enforce governance standards or provide an end user of your platform with limited access to the integrate.ai SDK, you can create **scoped API tokens**. Scoped tokens grant limited permissions, which enables you to control the level of access to trained sessions and models.\\n\\nIn the UI, you can view your personal API tokens as well as all scoped API tokens created in your organization's workspace through the SDK. These scoped user tokens are designed for use with the integrate.ai SDK. Tokens are tied to user identities through a unique ID, which is logged with each user action.\\n\\nLimiting user access by token greatly reduces the security risk of leaked credentials. For example, with an unscoped API token, a user could run tasks on a remote machine, where there is a risk that it could be leaked or exposed because it is shared in an outside (non-local) environment. To mitigate that risk, you can instead provide the user with a scoped token that has limited permissions and a short lifespan (maximum 30 days).\\n\\n \\n\\nCreate an unscoped token\\n\\nAs the user who manages other users' acce\\nss, you must first create your own unscoped token.\\n\\n1. Log in to your integrate.ai account on the web. \\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the access token and save it to a secure location. \\n\\nThis is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as .\\n\\n\", 'title': 'input/train-overview.md'}\n",
      "{'score': '0.3519156', 'text': '\\n\\nVerify user and token through the UI\\n\\nTo confirm that the user and token were created successfully, you can also view them in the web dashboard. \\n\\n1. Log in to the web dashboard.\\n2. Click Token Management.\\n2. Click User Scoped Tokens.\\n3. Locate the user name for the user you created. \\n\\n', 'title': 'input/user-auth.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"How do I renew my token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Differential privacy parameters can be specified during session creation, within the model configuration. \n",
       "\n",
       "The following code example shows how to set the differential privacy parameter for a session: \n",
       "\n",
       "```python\n",
       "session_config = SessionConfig(name=\"My Session\",\n",
       "                               differential_privacy_parameters=DifferentialPrivacyParameters(epsilon=0.1))\n",
       "```\n",
       "\n",
       "The `epsilon` parameter is the privacy budget, which is the maximum amount of privacy loss that is allowed. The higher the value, the less privacy is preserved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3922711', 'text': '\\n\\nControl Plane\\n\\nIn this deployment scenario, the integrate.ai system manages all components hosted in the customer\\'s environment using a limited permission role granted by the customer. \\n\\n \\n\\nDifferential Privacy\\n\\nWhat is differential privacy?\\nDifferential privacy is a technique for providing a provable measure of how “private” a data set can be. This is achieved by adding a certain amount of noise when responding to a query on the data. A balance needs to be struck between adding too much noise (making the computation less useful), and too little (reducing the privacy of the underlying data).\\nThe technique introduces the concept of a privacy-loss parameter (typically represented by ε (epsilon)), which can be thought of as the amount of noise to add for each invocation of some computation on data. A related concept is the privacy budget, which can be chosen by the data curator.\\nThis privacy budget represents the measurable amount of information exposed by the computation before the level of privacy is deemed unacceptable.\\nThe benefit of this approach is that by providing a quantifiable measure, there can be a guarantee about how “private” the release of information is. However, in practice, relating the actual privacy to the computation in question can be difficult: i.e. how private is private enough? What will ε need to be? These are open questions in practice for practitioners when applying DP to an application.\\nHow is Differential Privacy used in integrate.ai?\\nUsers can add Differential Privacy to any model built in integrate.ai. DP parameters can be specified during session creation, within the model configuration. \\nWhen is the right time to use Differential Privacy?\\nOverall, differential privacy can be best applied when there is a clear ability to select the correct ε for the computation, and/or it is acceptable to specify a large enough privacy loss budget to satisfy the computation needs.\\n\\n\\n \\n\\nEDA Intersect\\n\\n\\nThe Exploratory Data Analysis (EDA) Intersect feature for private record linkage (PRL) sessions enables you to access summary statistics about a group of datasets without needing access to the data itself. This allows you to get a basic understanding of the dataset when you don\\'t have access to the data or you are not allowed to do any computations on the data. It enables you to understand more about the intersection between two datasets.\\nEDA is an important pre-step for federated modelling and a simple form of federated analytics. The feature has a built in differential privacy setting. Differential privacy (DP) is dynamically added to each histogram that is generated for each feature in a participating dataset. The added privacy protection causes slight noise in the end result. \\nAt a high level, the process is similar to that of creating and running a session to train a model. The steps are:\\nAuthenticate with your access token.\\nConfigure an EDA session.\\nCreate and start the session.\\nRun the session and poll for session status.\\nAnalyze the datasets.\\nThe sample notebook (integrateai_eda_intersect_batch.ipynb) is an interactive tool for exploring the API, including the EDA feature, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\nAPI Reference\\nThe core API module that contains the EDA-specific functionality is integrate_ai_sdk.api.eda. This module includes a core object called EdaResults, which contains the results of an EDA session. \\nIf you are comfortable working with the integrate.ai SDK and API, see the API Documentation for details. \\n\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the notebook sample code. \\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\nConfigure an EDA Session\\nTo begin exploratory data analysis, you must first create a session, the same as you would for training a model. To configure the session, specify the following:\\nEDA data configuration (eda_data_config)\\nEDA configuration (eda_config)\\nprl_session_id for the PRL session you want to work with\\nThe eda_data_config specifies the names of the datasets used to generate the intersection in the PRL session in the format dataset_name : columns. If columns is empty ([]), then EDA is performed on all columns. \\nExclude any columns that were used as identifiers during the PRL sessions.\\nThe eda_config specifies the parameters for the EDA process, such as the strategy. \\nYou must also specify the session ID of a successful PRL session using the datasets listed in the eda_data_config. \\nExample:\\neda_data_config = {\"prl_sil0\":[],\"prl_silo1\":[]} \\neda_config = {\"strategy\": {\"name\": \"EDAHistogram\", \"params\": {}}}\\nprl_session_id = \"\"\\nCreate and start an EDA session\\nThe following code sample demonstrates creating and starting an EDA session to perform privacy-preserving data analysis on the intersection of two distinct datasets. It returns an EDA session ID that you can use to track and reference your session.\\neda_session = client.create_eda_session(\\n    name=\"EDA Intersect Session\",\\n    description=\"Testing EDA Intersect mode\",\\n    data_config=eda_data_config,\\n    eda_mode=\"intersect\",  #Generates histograms on an overlap of two distinct datasets\\n    prl_session_id=prl_session_id\\n).start()\\n\\neda_session.id\\nFor more information, see the create_eda_session() definition in the API documentation.\\nStart an EDA session using AWS Batch\\n', 'title': 'input/deployment.md'}\n",
      "{'score': '0.42838115', 'text': '\\n\\nPrivacy\\n\\nDP-SGD (differentially private stochastic gradient descent) is applied as an additional privacy-enhancing technology. The basic idea of this approach is to modify the gradients used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. \\n\\nFor more information, see .\\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.46061856', 'text': '\\n\\nPrivacy\\n\\nDP-SGD (differentially private stochastic gradient descent) is applied as an additional privacy-enhancing technology. The basic idea of this approach is to modify the gradients used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. \\n\\nFor more information, see .\\n\\n\\n\\nReferences [1]: https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression \\n', 'title': 'input/iai_glm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.36366454', 'text': '\\n\\nControl Plane\\n\\nIn this deployment scenario, the integrate.ai system manages all components hosted in the customer\\'s environment using a limited permission role granted by the customer. \\n\\n \\n\\nDifferential Privacy\\n\\nWhat is differential privacy?\\nDifferential privacy is a technique for providing a provable measure of how “private” a data set can be. This is achieved by adding a certain amount of noise when responding to a query on the data. A balance needs to be struck between adding too much noise (making the computation less useful), and too little (reducing the privacy of the underlying data).\\nThe technique introduces the concept of a privacy-loss parameter (typically represented by ε (epsilon)), which can be thought of as the amount of noise to add for each invocation of some computation on data. A related concept is the privacy budget, which can be chosen by the data curator.\\nThis privacy budget represents the measurable amount of information exposed by the computation before the level of privacy is deemed unacceptable.\\nThe benefit of this approach is that by providing a quantifiable measure, there can be a guarantee about how “private” the release of information is. However, in practice, relating the actual privacy to the computation in question can be difficult: i.e. how private is private enough? What will ε need to be? These are open questions in practice for practitioners when applying DP to an application.\\nHow is Differential Privacy used in integrate.ai?\\nUsers can add Differential Privacy to any model built in integrate.ai. DP parameters can be specified during session creation, within the model configuration. \\nWhen is the right time to use Differential Privacy?\\nOverall, differential privacy can be best applied when there is a clear ability to select the correct ε for the computation, and/or it is acceptable to specify a large enough privacy loss budget to satisfy the computation needs.\\n\\n\\n \\n\\nEDA Intersect\\n\\n\\nThe Exploratory Data Analysis (EDA) Intersect feature for private record linkage (PRL) sessions enables you to access summary statistics about a group of datasets without needing access to the data itself. This allows you to get a basic understanding of the dataset when you don\\'t have access to the data or you are not allowed to do any computations on the data. It enables you to understand more about the intersection between two datasets.\\nEDA is an important pre-step for federated modelling and a simple form of federated analytics. The feature has a built in differential privacy setting. Differential privacy (DP) is dynamically added to each histogram that is generated for each feature in a participating dataset. The added privacy protection causes slight noise in the end result. \\nAt a high level, the process is similar to that of creating and running a session to train a model. The steps are:\\nAuthenticate with your access token.\\nConfigure an EDA session.\\nCreate and start the session.\\nRun the session and poll for session status.\\nAnalyze the datasets.\\nThe sample notebook (integrateai_eda_intersect_batch.ipynb) is an interactive tool for exploring the API, including the EDA feature, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\nAPI Reference\\nThe core API module that contains the EDA-specific functionality is integrate_ai_sdk.api.eda. This module includes a core object called EdaResults, which contains the results of an EDA session. \\nIf you are comfortable working with the integrate.ai SDK and API, see the API Documentation for details. \\n\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the notebook sample code. \\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\nConfigure an EDA Session\\nTo begin exploratory data analysis, you must first create a session, the same as you would for training a model. To configure the session, specify the following:\\nEDA data configuration (eda_data_config)\\nEDA configuration (eda_config)\\nprl_session_id for the PRL session you want to work with\\nThe eda_data_config specifies the names of the datasets used to generate the intersection in the PRL session in the format dataset_name : columns. If columns is empty ([]), then EDA is performed on all columns. \\nExclude any columns that were used as identifiers during the PRL sessions.\\nThe eda_config specifies the parameters for the EDA process, such as the strategy. \\nYou must also specify the session ID of a successful PRL session using the datasets listed in the eda_data_config. \\nExample:\\neda_data_config = {\"prl_sil0\":[],\"prl_silo1\":[]} \\neda_config = {\"strategy\": {\"name\": \"EDAHistogram\", \"params\": {}}}\\nprl_session_id = \"\"\\nCreate and start an EDA session\\nThe following code sample demonstrates creating and starting an EDA session to perform privacy-preserving data analysis on the intersection of two distinct datasets. It returns an EDA session ID that you can use to track and reference your session.\\neda_session = client.create_eda_session(\\n    name=\"EDA Intersect Session\",\\n    description=\"Testing EDA Intersect mode\",\\n    data_config=eda_data_config,\\n    eda_mode=\"intersect\",  #Generates histograms on an overlap of two distinct datasets\\n    prl_session_id=prl_session_id\\n).start()\\n\\neda_session.id\\nFor more information, see the create_eda_session() definition in the API documentation.\\nStart an EDA session using AWS Batch\\n', 'title': 'input/deployment.md'}\n",
      "{'score': '0.45050406', 'text': '\\n\\nSet your AWS Credentials if you are generating temporary ones, else use the default profile credentials\\naws_creds = {\\n    \"ACCESS_KEY\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\\n    \"SECRET_KEY\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\\n    \"SESSION_TOKEN\": os.environ.get(\"AWS_SESSION_TOKEN\"),\\n    \"REGION\": os.environ.get(\"AWS_REGION\"),\\n}\\n```\\n\\n \\n\\nCreate an EDA Session for exploring the datasets\\n\\nTo create an EDA session, we specify a `dataset_config` dictionary indicating the columns to explore for each dataset. Here the empty list `[]` means to include all columns. \\n\\nFor information more information on how to configure an EDA session, see the documentation here.\\n\\n\\n```python\\neda_data_config = {\"prl_silo0\": [], \"prl_silo1\": []}\\neda_config = {\"strategy\": {\"name\": \"EDAHistogram\", \"params\": {}}}\\nprl_session_id = \"\"\\n```\\n\\n\\n```python\\neda_session = client.create_eda_session(\\n    name=\"Testing notebook - EDA Intersect session\",\\n    description=\"I am testing EDA on PRL session creation through a notebook\",\\n    data_config=eda_data_config,\\n    eda_mode=\"intersect\",\\n    prl_session_id=prl_session_id,\\n).start()\\n\\neda_session.id\\n```\\n\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n",
      "{'score': '0.45146665', 'text': '\\n\\nPrivacy\\n\\nDP-SGD (differentially private stochastic gradient descent) is applied as an additional privacy-enhancing technology. The basic idea of this approach is to modify the gradients used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. \\n\\nFor more information, see .\\n\\n', 'title': 'input/iai_ffnet.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"How do I set my differential privacy parameter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: This error indicates that there was an error in the service handler. This could be due to a misconfiguration of the service, or an issue with the network connection. Check the service configuration and network connection to ensure that everything is set up correctly. If the issue persists, contact your system administrator for assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.5782596', 'text': \"\\n\\nErrors\\n\\n\\nThis error section is stored in a separate file in includes/_errors.md. Slate allows you to optionally separate out your docs into many files...just save them to the includes folder and add them to the top of your index.md's frontmatter. Files are included in the order listed.\\n\\n\\nThe Kittn API uses the following error codes:\\n\\n\\nError Code | Meaning\\n---------- | -------\\n400 | Bad Request -- Your request is invalid.\\n401 | Unauthorized -- Your API key is wrong.\\n403 | Forbidden -- The kitten requested is hidden for administrators only.\\n404 | Not Found -- The specified kitten could not be found.\\n405 | Method Not Allowed -- You tried to access a kitten with an invalid method.\\n406 | Not Acceptable -- You requested a format that isn't json.\\n410 | Gone -- The kitten requested has been removed from our servers.\\n418 | I'm a teapot.\\n429 | Too Many Requests -- You're requesting too many kittens! Slow down!\\n500 | Internal Server Error -- We had a problem with our server. Try again later.\\n503 | Service Unavailable -- We're temporarily offline for maintenance. Please try again later.\\n\\n\", 'title': 'input/_errors.md'}\n",
      "{'score': '0.57931817', 'text': '\\n\\nClass name: IaiBaseMLTaskException\\n\\nFunctions: \\n, \\nDocumentation: \\n\\n\\nIaiBaseMLTaskException is an exception class used to indicate errors that occur when performing machine learning tasks. It is a subclass of the standard Python Exception class and provides additional information about the error that occurred. This class is used to provide a consistent way of handling errors in machine learning tasks. It can be used to provide more detailed information about the error, such as the type of error, the parameters that caused the error, and the stack trace of the error.\\n\\n', 'title': 'input/src/integrate_ai_sdk/exception.md'}\n",
      "{'score': '0.5809972', 'text': 'ReferenceData preparationModels- iai_ffnet- iai_glmPrivacySecurityAPI \\n\\nRelease Notes\\n\\n\\n15 Feb 2023: PRL Sessions\\nThis release introduces the ability  to perform private record linkage (PRL) on two datasets. \\nVersions:\\nSDK:\\nClient:\\nServer:\\nCLI Tool:\\n30 Jan 2023: User Authentication\\nThis release added two new features:\\nThe ability to train Gradient Boosted HFL Models. A guide is available here.\\nThe ability to generate scoped tokens for users. A guide is available here.\\nBug fixes:\\nClients may get disconnected from the server when training large models\\nVersions:\\nSDK: 0.5.36\\nClient: 2.0.18\\nServer: 2.2.19\\nCLI Tool: 0.0.38\\n08 Dec 2022: Integration with AWS Fargate\\nThis release added two new features:\\nThe ability to train Gradient Boosted HFL Models. A guide is available here.\\nThe ability to generate scoped tokens for users. A guide is available here.\\nVersions:\\nSDK: 0.5.36\\nClient: 2.0.18\\nServer: 2.2.19\\nCLI Tool: 0.0.38\\n08 Dec 2022: Integration with AWS Fargate\\nThis release introduces the ability to run an IAI training server on AWS Fargate through the integrate.ai SDK.  With an integrate.ai training server running on Fargate, your data in S3 buckets, and clients running on AWS Batch, you can use the SDK to manage and run fully remote training sessions.  A guide is available here.\\nVersions:\\nSDK: 0.5.13\\nClient: 2.0.11\\nCLI Tool: 0.0.33\\n02 Nov 2022: Integration with AWS Batch\\nThis release introduces the ability to run AWS Batch jobs through the integrate.ai SDK. Building on the previous release, which added support for data hosted in S3 buckets, you can now start and run a remote training session on remote data with jobs managed by AWS Batch. A guide is available here.\\nFeatures:\\nAdded the ability to run the iai client through AWS Batch\\nAdded the ability for the iai client to retrieve a token through the IAI_TOKEN environment variable\\nAdded a version command for the iai client: iai client version\\nNote: Docker must be running for the version command to return a value.\\nAdded support for a new session \"pending\" status\\nBREAKING CHANGE: \\nSession status mapping in the SDK has been updated as follows:\\ncreated -> Created\\nstarted -> Running\\npending -> Pending\\nfailed -> Failed\\nsucceeded -> Completed\\ncanceled -> Canceled\\nBug fixes:\\nFixed an issue with small batch sizes\\nVersions:\\nSDK: 0.3.31 \\nClient: 1.0.15\\nCLI Tool: 0.0.31\\n06 Oct 2022: Exploratory Data Analysis & S3 Support\\nFeatures:\\nExploratory Data Analysis (EDA) - integrate.ai now supports the ability to generate histograms for each feature of a dataset. Use the results of the EDA session to calculate summary statistics for both continuous and categorical variables. See more about the feature here.\\nThis feature has Differential Privacy applied automatically to each histogram to add noise and reduce privacy leakage. The Differential Privacy settings are dynamic and applied to best suit each dataset individually, to ensure privacy protection without excessive noise.\\nS3 data path support - load data from an s3 bucket for the  and  commands. You can use S3 URLs as the data_path given that your AWS CLI environment is properly configured. Read more on how to configure this integration here.\\nClient logging via  command - this new feature in the integrate-ai CLI package allows a user to access session logs from clients, to be used as a tool to help debug failed sessions. Access this using the  command.\\nVersions:\\nSDK: 0.3.20\\nClient: 1.0.8\\nCLI Too: 0.0.21\\n\\n14 Sept 2022: Infrastructure upgrades for session abstraction \\nSDK Version: 0.3.5\\nClient Version: 1.0.2\\nCLI Tool Version: 0.0.21\\n\\n \\n\\nStrategy Library\\n\\nReference guide for available training strategies.\\n\\nFedAvg\\nFederated Averaging Strategy implemented based on https://arxiv.org/abs/1602.05629\\nParameters\\nfraction_fit (float, optional): Fraction of clients used during training. Defaults to 0.1. \\nfraction_eval (float, optional): Fraction of clients used during validation. Defaults to 0.1.\\nmin_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 1.\\nmin_eval_clients (int, optional): Minimum number of clients used during validation. Defaults to 1. \\nmin_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 1. \\neval_fn (Callable[[Weights], Optional[Tuple[float, float]]], optional): Function used for validation. Defaults to None. \\non_fit_config_fn (Callable[[int], Dict[str, Scalar]], optional): Function used to configure training. Defaults to None.\\non_evaluate_config_fn (Callable[[int], Dict[str, Scalar]], optional): Function used to configure validation. Defaults to None.\\naccept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True. initial_parameters (Weights, optional): Initial global model parameters.\\n\\nFedAvgM\\nFederated Averaging with Momentum (FedAvgM) strategy -https://arxiv.org/pdf/1909.06335.pdf\\n\\nSame params as FedAvg plus the following additional ones\\ninitial_parameters (Weights, optional): Initial global model parameters. \\nserver_learning_rate (float): Server-side learning rate used in server-side optimization. Defaults to 1.0, which is the same as the vanilla FedAvg \\nserver_momentum(float): Server-side momentum factor used for FedAvgM. Defaults to 0.0. \\nnesterov (bool): Enables Nesterov momentum. Defaults to False.\\n\\nFedAdam\\nAdaptive Federated Optimization using Adam (FedAdam) https://arxiv.org/abs/2003.00295\\n\\nSame Params as FedAvg plus the following:\\ninitial_parameters (Weights, optional): Initial global model parameters. \\neta (float, optional): Server-side learning rate. Defaults to 1e-1. \\nbeta_1 (float, optional): Momentum parameter. Defaults to 0.9. \\nbeta_2 (float, optional): Second moment parameter. Defaults to 0.99. \\ntau (float, optional): Controls the degree of adaptability for the algorithm. Defaults to 1e-3.\\n\\nFedAdagrad\\nAdaptive Federated Optimization using Adagrad (FedAdagrad)strategy https://arxiv.org/abs/2003.00295\\n\\nSame Params as FedAvg plus the following:\\n\\ninitial_parameters (Weights, optional): Initial global model parameters. \\neta (float, optional): Server-side learning rate. Defaults to 1e-1. \\nbeta_1 (float, optional): Momentum parameter. Defaults to 0.0. Note that typical AdaGrad does not use momentum, thus usually beta_1 is kept 0.0 \\ntau (float, optional): Controls the degree of adaptability for the algorithm. Defaults to 1e-3. Smaller tau means higher degree of adaptability of server-side learning rate.\\n\\nFedOpt\\nAdaptive Federated Optimization (FedOpt) abstract strategy https://arxiv.org/abs/2003.00295\\n\\nSame Params as FedAvg plus the following additional ones\\n\\ninitial_parameters (Weights, optional): Initial global model parameters.\\neta (float, optional): Server-side learning rate. Defaults to 1e-1. \\nbeta_1 (float, optional): Momentum parameter. Defaults to 0.0. \\nbeta_2 (float, optional): Second moment parameter. Defaults to 0.0. \\ntau (float, optional): Controls the algorithm\\'s degree of adaptability. Defaults to 1e-3. Smaller tau means higher degree of adaptability of server-side learning rate.\\n\\nFedYogi\\nFederated learning strategy using Yogi on server-side https://arxiv.org/abs/2003.00295v5\\n\\ninitial_parameters (Weights, optional): Initial global model parameters. \\neta (float, optional): Server-side learning rate. Defaults to 1e-1. \\nbeta_1 (float, optional): Momentum parameter. Defaults to 0.9. \\nbeta_2 (float, optional): Second moment parameter. Defaults to 0.99. \\ntau (float, optional): Controls the algorithm\\'s degree of adaptability. Defaults to 1e-3.\\n\\n\\n', 'title': 'input/reference.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.49679387', 'text': '\\n\\nClass name: IaiBaseMLTaskException\\n\\nFunctions: \\n, \\nDocumentation: \\n\\n\\nIaiBaseMLTaskException is an exception class used to indicate errors that occur when performing machine learning tasks. It is a subclass of the standard Python Exception class and provides additional information about the error that occurred. This class is used to provide a consistent way of handling errors in machine learning tasks. It can be used to provide more detailed information about the error, such as the type of error, the parameters that caused the error, and the stack trace of the error.\\n\\n', 'title': 'input/src/integrate_ai_sdk/exception.md'}\n",
      "{'score': '0.500468', 'text': '\\n\\nPrerequisites:\\nAn instance of our docker client, downloaded from the Docker page in the UI  \\nAn IAI token, created using the Token Management page in the UI\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.5202307', 'text': '\\n\\nInstall integrate.ai packages\\n\\n1. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: \\n`pip install integrate-ai`\\n2. Install the SDK package using the access token you generated.\\n`iai sdk install --token `\\n3. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container.\\n`iai client pull --token `\\n\\n*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment.\\n\\n\\n', 'title': 'input/install-sdk.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"\"\"What does this error mean:\n",
    "```2023-05-24 02:19:25,244 FLOUR MainThread ERROR | neural_net.py:215 | <_MultiThreadedRendezvous of RPC that terminated with:\n",
    "\tstatus = StatusCode.UNKNOWN\n",
    "\tdetails = \"Error in service handler!\"\n",
    "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:99.79.32.55:9999 {grpc_message:\"Error in service handler!\", grpc_status:2, created_time:\"2023-05-24T02:19:25.193640106+00:00\"}\"\n",
    ">\n",
    "05/24/2023 02:19:25:ERROR:<_MultiThreadedRendezvous of RPC that terminated with:\n",
    "\tstatus = StatusCode.UNKNOWN\n",
    "\tdetails = \"Error in service handler!\"\n",
    "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:99.79.32.55:9999 {grpc_message:\"Error in service handler!\", grpc_status:2, created_time:\"2023-05-24T02:19:25.193640106+00:00\"}\"\n",
    ">\n",
    "CLI0006: Unable to join server for Session 'dd5a90b700'```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AI: This error indicates that the AWS Access Key ID you provided is invalid or does not exist in the AWS records. Please check that the Access Key ID is correct and that it is associated with the correct AWS account."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.4605072', 'text': '\\n\\nThis parameter is used by the server start-up to extract the access token value from SSM\\nssm_token_key = \"sample_session_token\"\\nssm = boto3.client(\\'ssm\\', region_name=\\'\\')\\nssm.put_parameter(\\n    Name=ssm_token_key,\\n    Value=IAI_TOKEN,\\n    Overwrite=True,\\n    Type=\\'SecureString\\',\\n    KeyId=\\'\\' # Use your own KMS keyID here\\n)\\n```\\n\\n', 'title': 'input/aws-fargate-sdk.md'}\n",
      "{'score': '0.4925499', 'text': '\\n\\nAuthentication\\n\\nComplete the steps to create an IAI access token, hereafter referred to as .\\n\\nOn the AWS CLI, set the token as a parameter for your SSM agent. SSM handles getting and using the token as needed for the batch session.\\n\\n```python\\naws ssm put-parameter --name iai-token --value  --type SecureString\\n```\\n\\nExample response:\\n\\n```\\n{\\n    \"Version\": 1,\\n    \"Tier\": \"Standard\"\\n}\\n```\\n\\n \\n\\nAbout \"secrets\"\\n\\nIn order for the batch job to access the integrate.ai JWT through SSM it needs to be set in the job definition secrets configuration. \\nSet the name of the secret to IAI_TOKEN to create the IAI_TOKEN environment variable that the docker client uses to authenticate with the session. \\nThe valueFrom is the SSM key that contains the integrate.ai access token. If you are running the batch job in the same region that the SSM parameter was created in, pass in the name of the SSM parameter. If the SSM parameter is in a different region, pass in the SSM parameter ARN. \\nTo have different tokens for different user groups, you must have a different batch job definition for each token.\\nThe command and parameter values are examples only. They are overwritten when starting a batch job with the SDK. \\nBatch job configuration is now complete. \\n\\n\\n \\n\\nCreate an AWS KMS Key\\n\\nIf you do not already have one, define a symmetric key for encryption and decryption in AWS KMS. The IAI server uses the ID of your key to retrieve the IAI access token through SSM. \\n\\nIn the KMS console, click Create a Key.\\nKeep the default settings for Symmetric and Encrypt and decrypt. Click Next.\\nType an Alias for the key. Click Next.\\nSelect one or more Key administrators and click Next.\\nSelect one or more IAM Users and/or Roles to grant access to the key. \\nThe SDK User IAM Role must have GenerateDataKey and Encrypt permissions. \\nThe Fargate Task and/or Batch job roles must have Decrypt permission.\\nClick Next, then click Finish. \\nMake a note of the Key ID. This parameter is required as part of the training session definition. \\n\\n\\n', 'title': 'input/aws-batch-manual.md'}\n",
      "{'score': '0.5161065', 'text': \"\\n\\nErrors\\n\\n\\nThis error section is stored in a separate file in includes/_errors.md. Slate allows you to optionally separate out your docs into many files...just save them to the includes folder and add them to the top of your index.md's frontmatter. Files are included in the order listed.\\n\\n\\nThe Kittn API uses the following error codes:\\n\\n\\nError Code | Meaning\\n---------- | -------\\n400 | Bad Request -- Your request is invalid.\\n401 | Unauthorized -- Your API key is wrong.\\n403 | Forbidden -- The kitten requested is hidden for administrators only.\\n404 | Not Found -- The specified kitten could not be found.\\n405 | Method Not Allowed -- You tried to access a kitten with an invalid method.\\n406 | Not Acceptable -- You requested a format that isn't json.\\n410 | Gone -- The kitten requested has been removed from our servers.\\n418 | I'm a teapot.\\n429 | Too Many Requests -- You're requesting too many kittens! Slow down!\\n500 | Internal Server Error -- We had a problem with our server. Try again later.\\n503 | Service Unavailable -- We're temporarily offline for maintenance. Please try again later.\\n\\n\", 'title': 'input/_errors.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.41999924', 'text': '\\n\\nAuthentication\\n\\nComplete the steps to create an IAI access token, hereafter referred to as .\\n\\nOn the AWS CLI, set the token as a parameter for your SSM agent. SSM handles getting and using the token as needed for the batch session.\\n\\n```python\\naws ssm put-parameter --name iai-token --value  --type SecureString\\n```\\n\\nExample response:\\n\\n```\\n{\\n    \"Version\": 1,\\n    \"Tier\": \"Standard\"\\n}\\n```\\n\\n \\n\\nAbout \"secrets\"\\n\\nIn order for the batch job to access the integrate.ai JWT through SSM it needs to be set in the job definition secrets configuration. \\nSet the name of the secret to IAI_TOKEN to create the IAI_TOKEN environment variable that the docker client uses to authenticate with the session. \\nThe valueFrom is the SSM key that contains the integrate.ai access token. If you are running the batch job in the same region that the SSM parameter was created in, pass in the name of the SSM parameter. If the SSM parameter is in a different region, pass in the SSM parameter ARN. \\nTo have different tokens for different user groups, you must have a different batch job definition for each token.\\nThe command and parameter values are examples only. They are overwritten when starting a batch job with the SDK. \\nBatch job configuration is now complete. \\n\\n\\n \\n\\nCreate an AWS KMS Key\\n\\nIf you do not already have one, define a symmetric key for encryption and decryption in AWS KMS. The IAI server uses the ID of your key to retrieve the IAI access token through SSM. \\n\\nIn the KMS console, click Create a Key.\\nKeep the default settings for Symmetric and Encrypt and decrypt. Click Next.\\nType an Alias for the key. Click Next.\\nSelect one or more Key administrators and click Next.\\nSelect one or more IAM Users and/or Roles to grant access to the key. \\nThe SDK User IAM Role must have GenerateDataKey and Encrypt permissions. \\nThe Fargate Task and/or Batch job roles must have Decrypt permission.\\nClick Next, then click Finish. \\nMake a note of the Key ID. This parameter is required as part of the training session definition. \\n\\n\\n', 'title': 'input/aws-batch-manual.md'}\n",
      "{'score': '0.44681984', 'text': '\\n\\nThis parameter is used by the server start-up to extract the access token value from SSM\\nssm_token_key = \"sample_session_token\"\\nssm = boto3.client(\\'ssm\\', region_name=\\'\\')\\nssm.put_parameter(\\n    Name=ssm_token_key,\\n    Value=IAI_TOKEN,\\n    Overwrite=True,\\n    Type=\\'SecureString\\',\\n    KeyId=\\'\\' # Use your own KMS keyID here\\n)\\n```\\n\\n', 'title': 'input/aws-fargate-sdk.md'}\n",
      "{'score': '0.4849566', 'text': '\\n\\nRequirements\\n\\nThis section outlines the setup steps required to configure your working environment. Steps that are performed in the AWS platform are not explained in detail. Refer to the AWS documentation as needed. \\n\\nThe requirements are tool-agnostic - that is, you can complete the steps through the AWS console, or through a tool such as Terraform or AWS CloudFormation. \\n\\n', 'title': 'input/aws-batch-manual.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"\"\"What does this error mean:\n",
    "```An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: The best way to get the model weights from an HFL logistic regression analysis is to use the `get_weights()` method. This method returns a dictionary containing the weights for each feature in the model. \n",
       "\n",
       "```python\n",
       "weights = model.get_weights()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.4343601', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "{'score': '0.46388203', 'text': '\\n\\nGradient Boosted Models (HFL-GBM)\\n\\nGradient boosting is a machine learning algorithm for building predictive models that helps minimize the bias error of the model. The gradient boosting model provided by integrate.ai is an HFL model that uses the sklearn implementation of HistGradientBoostingClassifier for classifier tasks and HistGradientBoostingRegresssor for regression tasks.\\n\\nThe GBM sample notebook (integrateai_api_gbm.ipynb) provides sample code for running the SDK, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n', 'title': 'input/hfl-gbm.md'}\n",
      "{'score': '0.4657126', 'text': '\\n\\nHFL FFNet\\n\\nThe iai_ffnet model is a feedforward neural network for horizontal federated learning (HFL) that uses the same activation for each hidden layer.\\n\\nThis model only supports classification and regression. Custom loss functions are not supported. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.513057', 'text': '\\n\\nEvaluation Metrics\\n\\nWhen the session is complete, you can see a set of metrics for all rounds of training, as well as metrics for the final model.\\nRetrieve Metrics for a Session\\nUse the SessionMetrics class of the API to store and retrieve metrics for a session. You can retrieve the model performance metrics as a dictionary (Dict), or plot them. See the API class reference for details. \\nTypical usage example:\\nclient = connect(\"token\") \\n\\nalready_trained_session_id = \"\"\\n\\nsession = client.fl_session(already_trained_session_id)\\n\\n \\n\\nretrieve the metrics for the session as a dictionary\\nmetrics = session.metrics.as_dict()\\nAuthenticate to and connect to the integrate.ai client.\\nProvide the session ID that you want to retrieve the metrics for as the already_trained_session_id.\\nCall the SessionMetrics class.\\n\\nAvailable Metrics \\nThe Federated Loss value for the latest round of model training is reported as the global_model_federated_loss(float) attribute for an instance of SessionMetrics. \\nThis is a model level metric reported for each round of training. It is a weighted average loss across different clients, weighted by the number of examples/samples from each silo. \\nSee the metrics by machine learning task in the following table:\\n\\n| Classification and Logistic | Regression and Normal | Poisson, Gamma, Inverse Gaussian |\\n| Loss (cross-entropy) | Loss (mean squared error) | Loss (unit deviance) |\\n| ROC-AUC | R2 score | R2 score |\\n| Accuracy | | |\\n\\n\\n \\n\\nGlossary\\n\\nActive Party - In VFL, the party that owns the labels. Might also be contributing data.\\nAggregator - the integrate.ai cloud server that collects and aggregates models. \\nCentral server - the integrate.ai cloud server. Also known as the aggregator. The central server does not collect or host datasets. \\nClient - the integrate.ai client software package. \\nData custodian - the user in charge of the dataset or data silo. May or may not also be a machine learning scientist. \\nDataset or data  silo - a single unique collection of data.\\nDifferential privacy - a technique that adds noise to the model during local training to reduce the possibility that the model can be used to re-identify individual data points.\\nEpoch - one cycle or iteration of training a complete dataset. In integrate.ai terms, one epoch is one round. \\nFederated machine learning - a machine learning technique where model training is performed on datasets on local clients with the model parameters being aggregated on a central server. \\nFeedforward Neural Network (FFN) - a type of neural network in which information flows through the nodes in a single direction (forward). \\nExamples of use cases include: \\nClassification tasks like image recognition or churn conversion prediction.\\nRegression tasks like forecasting revenues and expenses, or determining the relationship between drug dosage and blood pressure\\nGradient Boosted Models (GBM) - a model class that builds predictive models by using three elements: a loss function, weak learners, and an additive model where trees are added one at a time, and existing trees in the model are not changed.\\nGeneralized Linear Models (GLM) - a model class that supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression, and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\nExamples of use cases include:\\nAgriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\nRisk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\nPredictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\nHFL - Horizontal federated learning. Also known as sample-based federated learning. \\nMachine Learning (Data) Scientist - the user most often responsible for training the data model. May or may not be a custodian for one or more datasets. \\nModel - a file that has been trained to recognize certain types of patterns.\\nNode - integrate.ai term for a single dataset associated with a single data collector or machine. The combination of dataset and machine together form a node.  \\nPassive Party - In VFL, the party that is contributing data only.\\nPET - Privacy Enhancing Technology\\nPrivate Record Linkage (PRL) - \\nRound - integrate.ai term for one epoch of training with a complete dataset.\\nSession - integrate.ai term for the time period in which rounds of model training are being performed.\\nTraining - the process of generating a model (file) that can recognize patterns in datasets. \\nVFL - Vertical federated learning. A federated learning setting where multiple parties, each having different features for the same user data set, jointly train machine learning models without sharing their data or model parameters. \\n\\n\\n\\n \\n\\nExploratory Data Analysis (HFL)\\n\\nThe Exploratory Data Analysis (EDA) feature for horizontal federated learning (HFL) enables you to access summary statistics about a group of datasets without needing access to the data itself. This allows you to get a basic understanding of the dataset when you don\\'t have access to the data or you are not allowed to do any computations on the data.\\n\\nEDA is an important pre-step for federated modelling and a simple form of federated analytics. The feature has a built in differential privacy setting. Differential privacy (DP) is dynamically added to each histogram that is generated for each feature in a participating dataset. The added privacy protection causes slight noise in the end result. \\n\\nAt a high level, the process is similar to that of creating and running a session to train a model. The steps are:\\n\\n1. Authenticate with your access token.\\n2. Configure an EDA session.\\n3. Create and start the session.\\n4. Run the session and poll for session status.\\n5. Analyze the datasets.\\n\\nThe sample notebook (*integrate_ai_api.ipynb*) provides runnable code examples for exploring the API, including the EDA feature, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n \\n\\nAPI Reference\\n\\nThe core API module that contains the EDA-specific functionality is integrate_ai_sdk.api.eda. This module includes a core object called EdaResults, which contains the results of an EDA session. \\n\\nf you are comfortable working with the integrate.ai SDK and API, see the API Documentation for details. \\nThis tutorial assumes that you have correctly configured your environment for working with integrate.ai, as described in .\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the notebook sample code. \\nReminder: You can generate and manage tokens through the integrate.ai web portal.\\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\n\\n\\n', 'title': 'input/evaluation-metrics.md'}\n",
      "{'score': '0.53092134', 'text': '\\n\\nHFL FFNet\\n\\nThe iai_ffnet model is a feedforward neural network for horizontal federated learning (HFL) that uses the same activation for each hidden layer.\\n\\nThis model only supports classification and regression. Custom loss functions are not supported. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.5373287', 'text': \"\\n\\nTrained model parameters are accessible from the completed session\\n\\nModel parameters can be retrieved using the model's as_sklearn method. \\n\\n\\n```python\\nmodel = training_session.model().as_sklearn()\\nmodel\\n```\\n\\n\", 'title': 'input/integrateai_api_gbm.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"What's the best way to get the model weights from an HFL logistic regression analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To set up multi-client training, you will need to split your data into two silos, one for each client. Each silo should contain the features that the client will use for training. For example, if one client has Y, X1 and the other has X2, the first silo should contain Y and X1, and the second silo should contain X2. \n",
       "\n",
       "You can then use the following commands to set up the clients for training: \n",
       "\n",
       "```python\n",
       "train_path1 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo0.parquet\"\n",
       "train_path2 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo1.parquet\"\n",
       "\n",
       "client_1 = subprocess.Popen(\n",
       "    f\"iai client train --token {IAI_TOKEN} --session {training_session.id} --train-path {train_path1} --test-path {data_path}/test.parquet --batch-size 1024 --client-"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.46817732', 'text': '\\n\\nRun Training Client jobs on AWS Batch\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.47253546', 'text': '\\n\\nRemote datasets\\n\\nOne of the key features of the integrate.ai platform is the ability to work with datasets without having to colocate them. \\nThe integrate.ai client and SDK are capable of working with datasets that are hosted remotely on AWS S3. You must set up and configure the AWS CLI to use S3 datasets.\\n\\n', 'title': 'input/data-requirements.md'}\n",
      "{'score': '0.48136765', 'text': '\\n\\nTest data path in s3\\ntrain_path1 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo0.parquet\"\\ntrain_path2 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo1.parquet\"\\ntest_path = \"s3://iai-client.sample-data-e2e.integrate.ai/test.parquet\"\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.29123285', 'text': '\\n\\nStart a training session using iai client\\nYou can use S3 URLs as `data_path` given that you AWS CLI environment is properly configured. The following environment variables have to be set for the `iai client` to be able to read S3 data locations:\\n```\\nexport AWS_ACCESS_KEY_ID=$access_id\\nexport AWS_SECRET_ACCESS_KEY=$secret_id\\nexport AWS_SESSION_TOKEN=$token\\nexport AWS_REGION=ca-central-1\\n```\\nReplace `$var` in the above snippet with actual values.\\n\\n\\n```python\\nimport subprocess\\nfrom os.path import abspath\\n\\n \\n\\nThe path to the data you want to train should be an absolute path to the directory\\ndata_path = abspath(dataset_path)\\n\\nclient_1 = subprocess.Popen(\\n    f\"iai client train --token {IAI_TOKEN} --session {session.id} --train-path {data_path} --test-path {data_path} --batch-size 512 --approve-custom-package --client-name client-1 --remove-after-complete\",\\n    shell=True,\\n    stdout=subprocess.PIPE,\\n    stderr=subprocess.PIPE,\\n)\\nclient_2 = subprocess.Popen(\\n    f\"iai client train --token {IAI_TOKEN} --session {session.id} --train-path {data_path} --test-path {data_path} --batch-size 512 --approve-custom-package --client-name client-2 --remove-after-complete\",\\n    shell=True,\\n    stdout=subprocess.PIPE,\\n    stderr=subprocess.PIPE,\\n)\\n```\\n\\n \\n\\nPoll for session status\\n\\nYou can log whatever you would like about the session during this time. For now we are logging the current round and the session status. If you want to access the logs later you can use `iai client log` command.\\n\\n\\n```python\\nimport time\\n\\ncurrent_round = None\\ncurrent_status = None\\nwhile client_1.poll() is None or client_2.poll() is None:\\n    output1 = client_1.stdout.readline().decode(\"utf-8\").strip()\\n    output2 = client_2.stdout.readline().decode(\"utf-8\").strip()\\n    if output1:\\n        print(\"silo1: \", output1)\\n    if output2:\\n        print(\"silo2: \", output2)\\n\\n    # poll for status and round\\n    if current_status != session.status:\\n        print(\"Session status: \", session.status)\\n        current_status = session.status\\n    if current_round != session.round and session.round > 0:\\n        print(\"Session round: \", session.round)\\n        current_round = session.round\\n    time.sleep(1)\\n\\noutput1, error1 = client_1.communicate()\\noutput2, error2 = client_2.communicate()\\n\\nprint(\\n    \"client_1 finished with return code: %d\\\\noutput: %s\\\\n  %s\"\\n    % (client_1.returncode, output1.decode(\"utf-8\"), error1.decode(\"utf-8\"))\\n)\\nprint(\\n    \"client_2 finished with return code: %d\\\\noutput: %s\\\\n  %s\"\\n    % (client_2.returncode, output2.decode(\"utf-8\"), error2.decode(\"utf-8\"))\\n)\\n```\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.32545704', 'text': '\\n\\nTest data path in s3\\ntrain_path1 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo0.parquet\"\\ntrain_path2 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo1.parquet\"\\ntest_path = \"s3://iai-client.sample-data-e2e.integrate.ai/test.parquet\"\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.33406156', 'text': '\\n\\nCreate a Training Session\\n\\nThe documentation for creating a session gives a bit more context into the parameters that are used during training session creation.\\nFor this session we are going to be using two training clients and two rounds. \\n\\n\\n```python\\ntraining_session = client.create_fl_session(\\n    name=\"Testing notebook\",\\n    description=\"I am testing session creation through a notebook\",\\n    min_num_clients=2,\\n    num_rounds=2,\\n    package_name=\"iai_ffnet\",\\n    model_config=model_config,\\n    data_config=data_schema,\\n).start()\\n\\ntraining_session.id\\n```\\n\\n \\n\\nStart a training session using iai client\\nMake sure that the sample data you downloaded to Start an EDA Session is saved to your `~/Downloads` directory, otherwise update the `data_path` below to point to the sample data.\\n\\n\\n```python\\nimport subprocess\\n\\ndata_path = \"~/Downloads/synthetic\"\\n\\nclient_1 = subprocess.Popen(\\n    f\"iai client train --token {IAI_TOKEN} --session {training_session.id} --train-path {data_path}/train_silo0.parquet --test-path {data_path}/test.parquet --batch-size 1024 --client-name client-1 --remove-after-complete\",\\n    shell=True,\\n    stdout=subprocess.PIPE,\\n    stderr=subprocess.PIPE,\\n)\\n\\nclient_2 = subprocess.Popen(\\n    f\"iai client train --token {IAI_TOKEN} --session {training_session.id} --train-path {data_path}/train_silo1.parquet --test-path {data_path}/test.parquet --batch-size 1024 --client-name client-2 --remove-after-complete\",\\n    shell=True,\\n    stdout=subprocess.PIPE,\\n    stderr=subprocess.PIPE,\\n)\\n```\\n\\n \\n\\nPoll for session status\\n\\nYou can log whatever you would like about the session during this time. For now we are logging the current round and the session status. If you want to access the logs later you can use `iai client log` command.\\n\\n\\n```python\\nimport time\\n\\ncurrent_round = None\\ncurrent_status = None\\nwhile client_1.poll() is None or client_2.poll() is None:\\n    output1 = client_1.stdout.readline().decode(\"utf-8\").strip()\\n    output2 = client_2.stdout.readline().decode(\"utf-8\").strip()\\n    if output1:\\n        print(\"silo1: \", output1)\\n    if output2:\\n        print(\"silo2: \", output2)\\n\\n    # poll for status and round\\n    if current_status != training_session.status:\\n        print(\"Session status: \", training_session.status)\\n        current_status = training_session.status\\n    if current_round != training_session.round and training_session.round > 0:\\n        print(\"Session round: \", training_session.round)\\n        current_round = training_session.round\\n    time.sleep(1)\\n\\noutput1, error1 = client_1.communicate()\\noutput2, error2 = client_2.communicate()\\n\\nprint(\\n    \"client_1 finished with return code: %d\\\\noutput: %s\\\\n  %s\"\\n    % (client_1.returncode, output1.decode(\"utf-8\"), error1.decode(\"utf-8\"))\\n)\\nprint(\\n    \"client_2 finished with return code: %d\\\\noutput: %s\\\\n  %s\"\\n    % (client_2.returncode, output2.decode(\"utf-8\"), error2.decode(\"utf-8\"))\\n)\\n```\\n\\n', 'title': 'input/integrateai_api.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"If I wanted to try multi-client training, where one client has Y, X1 and another client has X2 features, how would I setup the data schema and client train commands?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: You can use the integrate.ai API Sample Notebook to run a session on AWS Batch. \n",
       "\n",
       "1. Create an AWS Batch job queue. \n",
       "2. Create an AWS Batch compute environment. \n",
       "3. Create an AWS Batch job definition. \n",
       "4. Submit the job to the job queue. \n",
       "5. Monitor the job status. \n",
       "\n",
       "For more information, see the [AWS Batch Documentation](https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.22747159', 'text': '\\n\\nRun Training Client jobs on AWS Batch\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.2741452', 'text': '\\n\\nRun EDA Client jobs on AWS Batch\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.31078166', 'text': '\\n\\nintegrate.ai API Sample Notebook to run client on AWS Batch\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.16450123', 'text': '\\n\\nintegrate.ai API Sample Notebook to run client on AWS Batch\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.19896787', 'text': '\\n\\nintegrate.ai API Sample Notebook to run client on AWS Batch and AWS Fargate\\n\\n', 'title': 'input/integrateai_batch_client_vfl.md'}\n",
      "{'score': '0.19896787', 'text': '\\n\\nintegrate.ai API Sample Notebook to run client on AWS Batch and AWS Fargate\\n\\n', 'title': 'input/integrateai_fargate_batch_client_vfl.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"give me an example of running a session on AWS BATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: This is the constructor for the TemplateModel class. It is used to instantiate the model layers based on the configuration settings. It is called when an instance of the TemplateModel class is created. \n",
       "\n",
       "```python\n",
       "from integrate_ai_sdk.base_class import IaiBaseModule\n",
       "\n",
       "class TemplateModel(IaiBaseModule):\n",
       "    def __init__(self):\n",
       "        \"\"\"\n",
       "        Here you should instantiate your model layers based on the configs.\n",
       "        \"\"\"\n",
       "        super(TemplateModel, self).__init__()\n",
       "\n",
       "    def forward(self):\n",
       "        \"\"\"\n",
       "        The forward path of a model. Can take an input tensor and return a prediction tensor\n",
       "        \"\"\"\n",
       "        pass\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    template_model = TemplateModel()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.43804806', 'text': '\\n\\nCustom model, dataset, and LSTMTagger.json\\nChoose a name for your custom model, and set the path for the model and data configurations.  \\nNote that the name for your custom model **must be unique**.  \\nThis means that the name for your custom model cannot already be in the Package Name column of the Custom Models Packages Table in the Model Library Page of the UI.\\n\\n\\n\\n```python\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.47086468', 'text': '\\n\\nDownload the sample notebook\\n\\n\\n', 'title': 'input/install-sdk.md'}\n",
      "{'score': '0.48273015', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.2297841', 'text': '\\n\\nFunction name: __init__\\n\\nFunction: \\n```\\ndef __init__(self):\\n        \"\"\"\\n        Here you should instantiate your model layers based on the configs.\\n        \"\"\"\\n        super(TemplateModel, self).__init__()\\n```, \\nDocumentation: \\n\\n\\nThis is the constructor for the TemplateModel class. It is used to instantiate the model layers based on the configuration settings. It is called when an instance of the TemplateModel class is created.\\n\\n', 'title': 'input/src/integrate_ai_sdk/sample_packages/template_package/template_model.md'}\n",
      "{'score': '0.39327887', 'text': '\\n\\nFunction name: __init__\\n\\nFunction: \\n```\\ndef __init__(self, model: IaiBaseModule) -> None:\\n        \"\"\"Initializes IaiBaseMLTask.\\n\\n        Subclasses can overwrite the __init__ to pass in additional args if needed.\\n        \"\"\"\\n\\n        super().__init__()\\n        self.model = model\\n```, \\nDocumentation: \\n\\n\\nThis is the constructor for the IaiBaseMLTask class. It takes in an IaiBaseModule object as a parameter and sets it as the model for the class. Subclasses can overwrite the constructor to pass in additional arguments if needed.\\n\\n \\n\\nFunction name: forward\\n\\nFunction: \\n```\\ndef forward(self) -> torch.Tensor:\\n        \"\"\"Runs a forward pass of the model.\\n\\n        The forward function which is responsible for computations and returns the outputs of the model.\\n\\n        Returns:\\n            y_predict (Tensor): It should be a tensor of dimensions [N, C, l1, ...] where N is the batch size and\\n              C is the the number of classes and l1..n are the other dimensions, if your output consist of multiple\\n              dimensions.\\n        \"\"\"\\n```, \\nDocumentation: \\n\\nThis function runs a forward pass of the model, returning a tensor of dimensions [N, C, l1, ...], where N is the batch size, C is the number of classes, and l1..n are the other dimensions. The output of this function will be used for predictions.\\n\\n \\n\\nFunction name: _create_from_config\\n\\nFunction: \\n```\\ndef _create_from_config(cls, name: str, configs: Dict) -> Callable:\\n        \"\"\"Instantiates ML Task based on a JSON configuration.\\n\\n        The factory method which is used to instantiate custom ML tasks based on json config.\\n        The json config contains parameters that will be used to instantiate the IaiBaseMLTask class.\\n\\n        Args:\\n            name (str): Class name of the IaiBaseMLTask to be created.\\n            configs (Dict): Dict of params to be passed to the class instantiation.\\n\\n        Raises:\\n            IaiBaseMLTaskException: MLTask config should be a dictionary.\\n\\n        Returns:\\n            Callable: An instance of the task class.\\n        \"\"\"\\n\\n        if not isinstance(configs, dict):\\n            raise IaiBaseMLTaskException(\"MLTask config should be a dictionary.\")\\n        return find_sub_class(IaiBaseMLTask, name)(**configs)\\n```, \\nDocumentation: \\n\\n\\nThis function is used to instantiate custom ML tasks based on a JSON configuration. It takes two arguments, a class name and a dictionary of parameters, and returns an instance of the task class. The function will raise an IaiBaseMLTaskException if the configs argument is not a dictionary.\\n\\n \\n\\nFunction name: load_state_dict\\n\\nFunction: \\n```\\ndef load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True):\\n        \"\"\"Load model parameters (state_dict) into a model\\n\\n        Args:\\n            state_dict (dict): a dict containing parameters and persistent buffers.\\n            strict (bool, optional): whether to strictly enforce that the keys\\n                in :attr:`state_dict` match the keys returned by this module\\'s\\n                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\\n\\n        Returns:\\n            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\\n                * **missing_keys** is a list of str containing the missing keys\\n                * **unexpected_keys** is a list of str containing the unexpected keys\\n        Note:\\n            This is an IAI function based on PyTorch\\'s load_state_dict() method\\n        \"\"\"\\n\\n        # If the state_dict and the model do not match and strict is True, we are going to try the hypothesis\\n        # that the state_dict comes from a DP fixed model.\\n        # In the worst case in which the hypothesis is false, this will cause confusion, but the raised error\\n        # will remain the same.\\n        if self.state_dict().keys() != state_dict.keys() and strict and not ModuleValidator.is_valid(self):\\n            util.module_validator_fix(self)\\n            print(\"Differential privacy enabled. Incompatible modules were replaced with compatible counterparts\")\\n\\n        # should successfully return if the keys are matching for both the model and target_state_dict\\n        return super().load_state_dict(state_dict, strict)\\n```, \\nDocumentation: \\n\\n\\nThis function is used to load model parameters (state_dict) into a model. It takes in a state_dict (dict) containing parameters and persistent buffers, and a boolean value for strict. If the state_dict and the model do not match and strict is True, the function will try the hypothesis that the state_dict comes from a DP fixed model. If the hypothesis is false, this will cause confusion, but the raised error will remain the same. The function will return a NamedTuple with missing_keys and unexpected_keys fields.\\n\\n \\n\\nFunction name: __getitem__\\n\\nFunction: \\n```\\ndef __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\\n        \"\"\"Produces a Tuple of torch tensors for a given index.\\n\\n        Args:\\n            item (int): Index of datapoint to retrieve torch tensor for.\\n\\n        Returns:\\n            Tuple of tensors, and the last tuple index should be the data point label.\\n              integrate.ai will feed the rest of the tuple as input to the model.\\n        \"\"\"\\n```, \\nDocumentation: \\n\\nThis method returns a tuple of torch tensors for a given index. The last tuple index should be the data point label. The rest of the tuple will be fed as input to the model by integrate.ai. \\n\\nParameters: \\nitem (int): The index of the datapoint to retrieve the torch tensor for. \\n\\nReturns: \\nTuple of tensors, with the last tuple index being the data point label.\\n\\n', 'title': 'input/src/integrate_ai_sdk/base_class.md'}\n",
      "{'score': '0.39816147', 'text': '\\n\\nLoad the saved model\\n\\nTo load a model saved previously, a model object needs to be initialized first. This can be done by directly importing one of the IAI-supported packages (e.g., FFNet) or using the model class defined in a custom package. \\n\\n\\n```python\\nfrom integrate_ai_sdk.packages.FFNet.nn_model import FFNet\\n\\nmodel = FFNet(input_size=15, output_size=2, hidden_layer_sizes=[6, 6, 6])\\n\\n', 'title': 'input/integrateai_api.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"can you give the template for building a custom model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To deploy your model in AWS, you will need to set up an AWS Fargate task runner. This will allow you to run your model in the cloud. \n",
       "\n",
       "1. Create an Amazon ECR repository to store your model images. \n",
       "2. Create an IAM role for your task runner. This role will give your task runner permission to access your data. \n",
       "3. Create a task definition for your model. This will define the resources that your model will use. \n",
       "4. Create a task execution role for your task runner. This will give your task runner permission to access AWS services. \n",
       "5. Create a task runner to run your model. \n",
       "6. Start the task runner. \n",
       "7. Monitor the task runner to ensure that it is running correctly. \n",
       "\n",
       "For more detailed instructions, refer to the AWS Batch and Fargate Manual Setup guide."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3719529', 'text': '\\n\\nRequirements\\n\\nThis section outlines the setup steps required to configure your working environment. Steps that are performed in the AWS platform are not explained in detail. Refer to the AWS documentation as needed. \\n\\nThe requirements are tool-agnostic - that is, you can complete the steps through the AWS console, or through a tool such as Terraform or AWS CloudFormation. \\n\\n', 'title': 'input/aws-batch-manual.md'}\n",
      "{'score': '0.3770063', 'text': '\\n\\nRun Training Server on AWS Fargate\\n\\n', 'title': 'input/integrateai_fargate_server.md'}\n",
      "{'score': '0.37755996', 'text': '\\n\\nAWS Batch and Fargate Manual Setup\\n\\n', 'title': 'input/aws-batch-manual.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.24117789', 'text': '\\n\\nManaged Cloud-hosted\\n\\nFocused on model building, they don’t have IT team. They need job to run when they execute. Set up task runners - make sure AWS is up and running, there’s an ecr that can store the client image, and your batch job has permission to access your data. \\nWe set up server and manage it. \\n\\n\\n', 'title': 'input/deployment.md'}\n",
      "{'score': '0.26091376', 'text': '\\n\\nRunning a training server on AWS Fargate\\n\\nSet up the Fargate environment, as described in .\\n\\n', 'title': 'input/aws-fargate-sdk.md'}\n",
      "{'score': '0.28498834', 'text': '\\n\\nAWS Batch and Fargate Manual Setup\\n\\n', 'title': 'input/aws-batch-manual.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"How do I deploy this in AWS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: Integrate.ai supports the following evaluation metrics: accuracy, precision, recall, and F1 score. Additionally, the Federated Loss value for the latest round of model training is reported as the global_model_federated_loss(float) attribute for an instance of SessionMetrics. This is a model level metric reported for each round of training. It is a weighted average loss across different clients, weighted by the number of examples/samples from each silo. See the metrics by machine learning task in the following table:\n",
       "\n",
       "| Classification and Logistic | Regression and Normal | Poisson, Gamma, Inverse Gaussian |\n",
       "| Loss (cross-entropy) | Loss (mean squared error) | Loss (unit deviance) |\n",
       "| ROC-AUC | R2 score | R2 score |\n",
       "| Accuracy | | |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.50972795', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "{'score': '0.51639086', 'text': '\\n\\nDeployment Scenarios\\n\\n\\n', 'title': 'input/deployment.md'}\n",
      "{'score': '0.5164914', 'text': '\\n\\nEvaluation Metrics\\n\\nWhen the session is complete, you can see a set of metrics for all rounds of training, as well as metrics for the final model.\\nRetrieve Metrics for a Session\\nUse the SessionMetrics class of the API to store and retrieve metrics for a session. You can retrieve the model performance metrics as a dictionary (Dict), or plot them. See the API class reference for details. \\nTypical usage example:\\nclient = connect(\"token\") \\n\\nalready_trained_session_id = \"\"\\n\\nsession = client.fl_session(already_trained_session_id)\\n\\n \\n\\nretrieve the metrics for the session as a dictionary\\nmetrics = session.metrics.as_dict()\\nAuthenticate to and connect to the integrate.ai client.\\nProvide the session ID that you want to retrieve the metrics for as the already_trained_session_id.\\nCall the SessionMetrics class.\\n\\nAvailable Metrics \\nThe Federated Loss value for the latest round of model training is reported as the global_model_federated_loss(float) attribute for an instance of SessionMetrics. \\nThis is a model level metric reported for each round of training. It is a weighted average loss across different clients, weighted by the number of examples/samples from each silo. \\nSee the metrics by machine learning task in the following table:\\n\\n| Classification and Logistic | Regression and Normal | Poisson, Gamma, Inverse Gaussian |\\n| Loss (cross-entropy) | Loss (mean squared error) | Loss (unit deviance) |\\n| ROC-AUC | R2 score | R2 score |\\n| Accuracy | | |\\n\\n\\n \\n\\nGlossary\\n\\nActive Party - In VFL, the party that owns the labels. Might also be contributing data.\\nAggregator - the integrate.ai cloud server that collects and aggregates models. \\nCentral server - the integrate.ai cloud server. Also known as the aggregator. The central server does not collect or host datasets. \\nClient - the integrate.ai client software package. \\nData custodian - the user in charge of the dataset or data silo. May or may not also be a machine learning scientist. \\nDataset or data  silo - a single unique collection of data.\\nDifferential privacy - a technique that adds noise to the model during local training to reduce the possibility that the model can be used to re-identify individual data points.\\nEpoch - one cycle or iteration of training a complete dataset. In integrate.ai terms, one epoch is one round. \\nFederated machine learning - a machine learning technique where model training is performed on datasets on local clients with the model parameters being aggregated on a central server. \\nFeedforward Neural Network (FFN) - a type of neural network in which information flows through the nodes in a single direction (forward). \\nExamples of use cases include: \\nClassification tasks like image recognition or churn conversion prediction.\\nRegression tasks like forecasting revenues and expenses, or determining the relationship between drug dosage and blood pressure\\nGradient Boosted Models (GBM) - a model class that builds predictive models by using three elements: a loss function, weak learners, and an additive model where trees are added one at a time, and existing trees in the model are not changed.\\nGeneralized Linear Models (GLM) - a model class that supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression, and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\nExamples of use cases include:\\nAgriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\nRisk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\nPredictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\nHFL - Horizontal federated learning. Also known as sample-based federated learning. \\nMachine Learning (Data) Scientist - the user most often responsible for training the data model. May or may not be a custodian for one or more datasets. \\nModel - a file that has been trained to recognize certain types of patterns.\\nNode - integrate.ai term for a single dataset associated with a single data collector or machine. The combination of dataset and machine together form a node.  \\nPassive Party - In VFL, the party that is contributing data only.\\nPET - Privacy Enhancing Technology\\nPrivate Record Linkage (PRL) - \\nRound - integrate.ai term for one epoch of training with a complete dataset.\\nSession - integrate.ai term for the time period in which rounds of model training are being performed.\\nTraining - the process of generating a model (file) that can recognize patterns in datasets. \\nVFL - Vertical federated learning. A federated learning setting where multiple parties, each having different features for the same user data set, jointly train machine learning models without sharing their data or model parameters. \\n\\n\\n\\n \\n\\nExploratory Data Analysis (HFL)\\n\\nThe Exploratory Data Analysis (EDA) feature for horizontal federated learning (HFL) enables you to access summary statistics about a group of datasets without needing access to the data itself. This allows you to get a basic understanding of the dataset when you don\\'t have access to the data or you are not allowed to do any computations on the data.\\n\\nEDA is an important pre-step for federated modelling and a simple form of federated analytics. The feature has a built in differential privacy setting. Differential privacy (DP) is dynamically added to each histogram that is generated for each feature in a participating dataset. The added privacy protection causes slight noise in the end result. \\n\\nAt a high level, the process is similar to that of creating and running a session to train a model. The steps are:\\n\\n1. Authenticate with your access token.\\n2. Configure an EDA session.\\n3. Create and start the session.\\n4. Run the session and poll for session status.\\n5. Analyze the datasets.\\n\\nThe sample notebook (*integrate_ai_api.ipynb*) provides runnable code examples for exploring the API, including the EDA feature, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n \\n\\nAPI Reference\\n\\nThe core API module that contains the EDA-specific functionality is integrate_ai_sdk.api.eda. This module includes a core object called EdaResults, which contains the results of an EDA session. \\n\\nf you are comfortable working with the integrate.ai SDK and API, see the API Documentation for details. \\nThis tutorial assumes that you have correctly configured your environment for working with integrate.ai, as described in .\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the notebook sample code. \\nReminder: You can generate and manage tokens through the integrate.ai web portal.\\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\n\\n\\n', 'title': 'input/evaluation-metrics.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.3182329', 'text': '\\n\\nEvaluation Metrics\\n\\nWhen the session is complete, you can see a set of metrics for all rounds of training, as well as metrics for the final model.\\nRetrieve Metrics for a Session\\nUse the SessionMetrics class of the API to store and retrieve metrics for a session. You can retrieve the model performance metrics as a dictionary (Dict), or plot them. See the API class reference for details. \\nTypical usage example:\\nclient = connect(\"token\") \\n\\nalready_trained_session_id = \"\"\\n\\nsession = client.fl_session(already_trained_session_id)\\n\\n \\n\\nretrieve the metrics for the session as a dictionary\\nmetrics = session.metrics.as_dict()\\nAuthenticate to and connect to the integrate.ai client.\\nProvide the session ID that you want to retrieve the metrics for as the already_trained_session_id.\\nCall the SessionMetrics class.\\n\\nAvailable Metrics \\nThe Federated Loss value for the latest round of model training is reported as the global_model_federated_loss(float) attribute for an instance of SessionMetrics. \\nThis is a model level metric reported for each round of training. It is a weighted average loss across different clients, weighted by the number of examples/samples from each silo. \\nSee the metrics by machine learning task in the following table:\\n\\n| Classification and Logistic | Regression and Normal | Poisson, Gamma, Inverse Gaussian |\\n| Loss (cross-entropy) | Loss (mean squared error) | Loss (unit deviance) |\\n| ROC-AUC | R2 score | R2 score |\\n| Accuracy | | |\\n\\n\\n \\n\\nGlossary\\n\\nActive Party - In VFL, the party that owns the labels. Might also be contributing data.\\nAggregator - the integrate.ai cloud server that collects and aggregates models. \\nCentral server - the integrate.ai cloud server. Also known as the aggregator. The central server does not collect or host datasets. \\nClient - the integrate.ai client software package. \\nData custodian - the user in charge of the dataset or data silo. May or may not also be a machine learning scientist. \\nDataset or data  silo - a single unique collection of data.\\nDifferential privacy - a technique that adds noise to the model during local training to reduce the possibility that the model can be used to re-identify individual data points.\\nEpoch - one cycle or iteration of training a complete dataset. In integrate.ai terms, one epoch is one round. \\nFederated machine learning - a machine learning technique where model training is performed on datasets on local clients with the model parameters being aggregated on a central server. \\nFeedforward Neural Network (FFN) - a type of neural network in which information flows through the nodes in a single direction (forward). \\nExamples of use cases include: \\nClassification tasks like image recognition or churn conversion prediction.\\nRegression tasks like forecasting revenues and expenses, or determining the relationship between drug dosage and blood pressure\\nGradient Boosted Models (GBM) - a model class that builds predictive models by using three elements: a loss function, weak learners, and an additive model where trees are added one at a time, and existing trees in the model are not changed.\\nGeneralized Linear Models (GLM) - a model class that supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression, and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\nExamples of use cases include:\\nAgriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\nRisk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\nPredictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\nHFL - Horizontal federated learning. Also known as sample-based federated learning. \\nMachine Learning (Data) Scientist - the user most often responsible for training the data model. May or may not be a custodian for one or more datasets. \\nModel - a file that has been trained to recognize certain types of patterns.\\nNode - integrate.ai term for a single dataset associated with a single data collector or machine. The combination of dataset and machine together form a node.  \\nPassive Party - In VFL, the party that is contributing data only.\\nPET - Privacy Enhancing Technology\\nPrivate Record Linkage (PRL) - \\nRound - integrate.ai term for one epoch of training with a complete dataset.\\nSession - integrate.ai term for the time period in which rounds of model training are being performed.\\nTraining - the process of generating a model (file) that can recognize patterns in datasets. \\nVFL - Vertical federated learning. A federated learning setting where multiple parties, each having different features for the same user data set, jointly train machine learning models without sharing their data or model parameters. \\n\\n\\n\\n \\n\\nExploratory Data Analysis (HFL)\\n\\nThe Exploratory Data Analysis (EDA) feature for horizontal federated learning (HFL) enables you to access summary statistics about a group of datasets without needing access to the data itself. This allows you to get a basic understanding of the dataset when you don\\'t have access to the data or you are not allowed to do any computations on the data.\\n\\nEDA is an important pre-step for federated modelling and a simple form of federated analytics. The feature has a built in differential privacy setting. Differential privacy (DP) is dynamically added to each histogram that is generated for each feature in a participating dataset. The added privacy protection causes slight noise in the end result. \\n\\nAt a high level, the process is similar to that of creating and running a session to train a model. The steps are:\\n\\n1. Authenticate with your access token.\\n2. Configure an EDA session.\\n3. Create and start the session.\\n4. Run the session and poll for session status.\\n5. Analyze the datasets.\\n\\nThe sample notebook (*integrate_ai_api.ipynb*) provides runnable code examples for exploring the API, including the EDA feature, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n \\n\\nAPI Reference\\n\\nThe core API module that contains the EDA-specific functionality is integrate_ai_sdk.api.eda. This module includes a core object called EdaResults, which contains the results of an EDA session. \\n\\nf you are comfortable working with the integrate.ai SDK and API, see the API Documentation for details. \\nThis tutorial assumes that you have correctly configured your environment for working with integrate.ai, as described in .\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the notebook sample code. \\nReminder: You can generate and manage tokens through the integrate.ai web portal.\\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\n\\n\\n', 'title': 'input/evaluation-metrics.md'}\n",
      "{'score': '0.3679332', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.3787444', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"what evaluation metrics are supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: GLM stands for Generalized Linear Model. It is a model class that supports a variety of regression models, such as linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.31024972', 'text': '\\n\\nGLM\\n\\n#GBM\\n\\n#LSTM\\n\\n', 'title': 'input/train-overview.md'}\n",
      "{'score': '0.52677596', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.53406614', 'text': '\\n\\nGradient Boosted Models (HFL-GBM)\\n\\nGradient boosting is a machine learning algorithm for building predictive models that helps minimize the bias error of the model. The gradient boosting model provided by integrate.ai is an HFL model that uses the sklearn implementation of HistGradientBoostingClassifier for classifier tasks and HistGradientBoostingRegresssor for regression tasks.\\n\\nThe GBM sample notebook (integrateai_api_gbm.ipynb) provides sample code for running the SDK, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n', 'title': 'input/hfl-gbm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.3351254', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.3355331', 'text': '\\n\\nGLM\\n\\n#GBM\\n\\n#LSTM\\n\\n', 'title': 'input/train-overview.md'}\n",
      "{'score': '0.40161014', 'text': '\\n\\nGradient Boosted Models (HFL-GBM)\\n\\nGradient boosting is a machine learning algorithm for building predictive models that helps minimize the bias error of the model. The gradient boosting model provided by integrate.ai is an HFL model that uses the sklearn implementation of HistGradientBoostingClassifier for classifier tasks and HistGradientBoostingRegresssor for regression tasks.\\n\\nThe GBM sample notebook (integrateai_api_gbm.ipynb) provides sample code for running the SDK, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n', 'title': 'input/hfl-gbm.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"what does GLM mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: You can use the iai_glm model to train generalized linear models. The iai_glm model is a special case of single-layer neural nets with particular output activation functions. \n",
       "\n",
       "To train a GLM with integrate.ai, you need to create a model package that contains the model definition and the default model inputs. \n",
       "\n",
       "1. Create a folder to contain your custom model package. For this tutorial, this folder is named myCustomModel, and is located in the same parent folder as the template folder.\n",
       "\n",
       "Example path: C:\\<workspace>\\integrate_ai_sdk\\sample_packages\\myCustomModel\n",
       "\n",
       "2. Create two files in the custom model package folder: \n",
       "    a. `model.py` - the custom model definition. You can rename the template_model.py as a starting point for this file.\n",
       "    b. `<model-class-name>.json` - default model inputs for this model. It must have the same name as the model class name that is defined in the model.py file. \n",
       "\n",
       "If you are using the template files, the default name is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.36423275', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.36423275', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n",
      "{'score': '0.37011167', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.3297671', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.33968353', 'text': '\\n\\nLoad the saved model\\nTo load a model saved previously, a model object needs to be initialized first. This can be done by directly importing one of the IAI-supported packages (e.g., FFNet) or using the model class defined in a custom package.\\n\\n\\n```python\\nfrom integrate_ai_sdk.packages.GLM.model import GLM\\n\\nmodel = GLM(input_size=15, output_activation=\"sigmoid\")\\n\\n', 'title': 'input/integrateai_fargate_server.md'}\n",
      "{'score': '0.38429877', 'text': '\\n\\nHFL Model Training with a Sample Local Dataset\\n\\nAn end-to-end tutorial for how to train an existing model with a synthetic dataset on a local machine.\\n\\nTo help you get started, we\\'ve put together a tutorial based on synthetic data, with pre-built configuration files. In this tutorial, you will be training a federated feedforward neural network (iai_ffn) using data from two datasets. The datasets, model, and data configuration are provided for you. \\n\\nThe sample notebook (integrateai_api.ipynb) provides runnable code snippets for exploring the SDK, and should be used in parallel with this tutorial. This documentation provides supplementary and conceptual information to expand on the code demonstration.\\n\\n\\n \\n\\nPrerequisites\\n\\nComplete the for your local machine.\\nOpen the integrateai_api.ipynb notebook to test the code as you walk through this exercise.\\n\\nAuthenticate with your Access Token\\nBefore you can begin working with the system, you must authenticate to the API client with your access token.\\nSet an environment variable for your token (as described in Environment Setup) or replace it inline in the sample code. \\nReminder: You can generate and manage tokens through the integrate.ai web portal.\\nUse your access token to authenticate to the API client. The SDK simplifies the authentication process by providing a connect helper module. You import the connect helper from the SDK, provide your token, and authenticate to the client. \\nfrom integrate_ai_sdk.api import connect\\nclient = connect(token=IAI_TOKEN)\\n\\nUnderstanding Models\\nintegrate.ai has a standard model class available for Feedforward Neural Nets (iai_ffn) and Generalized Linear Models (iai_glm). These standard models are defined using JSON configuration files during session creation. \\nThe example below is a model provided by integrate.ai. \\nReview the sample model configuration\\nThe model configuration is a JSON object that contains the model parameters for the session. There are five main properties with specific key-value pairs used to configure the model: strategy, model, ml-task, optimizer, and differential_privacy_params. \\nFor this tutorial, you do not need to change any of the values. \\n\\n```json\\nmodel_config = {\\n    \"experiment_name\": \"test_synthetic_tabular\",\\n    \"experiment_description\": \"test_synthetic_tabular\",\\n    \"strategy\": {     \\n        \"name\": \"FedAvg\", //name of the federated learning strategy\\n        \"params\": {}\\n        },\\n    \"model\": {     // parameters specific to the model type \\n        \"params\": {\\n            \"input_size\": 15, \\n            \"hidden_layer_sizes\": [6, 6, 6], \\n            \"output_size\": 2\\n                   }\\n            },\\n    \"balance_train_datasets\": False, //performs undersampling on the dataset\\n    \"ml_task\": {        //specifies the federated learning strategy\\n        \"type\": \"classification\",\\n        \"params\": {\\n            \"loss_weights\": None,  //\\n        },\\n    },\\n    \"optimizer\": {\\n        \"name\": \"SGD\",     //name of the PyTorch optimizer used \\n        \"params\": {\\n            \"learning_rate\": 0.2,\\n            \"momentum\": 0.0}\\n            },\\n    \"differential_privacy_params\": {    //defines the differential privacy parameters\\n        \"epsilon\": 4, \\n        \"max_grad_norm\": 7\\n        },\\n    \"save_best_model\": {\\n        \"metric\": \"loss\",  // to disable this and save model from the last round, set to None\\n        \"mode\": \"min\",\\n    },\\n}\\n```\\n\\nReview the sample data configuration\\nThe data configuration is a JSON object where the user specifies predictor and target columns that are used to describe input data. This is the same structure for both GLM and FNN.\\n\\n```json\\ndata_config = {\\n    \"predictors\": [\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\"],\\n    \"target\": \"y\",\\n}\\n```\\n\\nNow that you\\'ve reviewed the model and data configuration, the next step is to create a training session to begin working with the model and datasets. \\n\\n \\n\\nCreate and start the training session\\n\\nFederated learning models created in integrate.ai are trained through sessions. You define the parameters required to train a federated model, including data and model configurations, in a session.\\n\\nCreate a session each time you want to train a new model.\\n\\nThe following code sample demonstrates creating and starting a session with two training clients (two datasets) and two rounds. It returns a session ID that you can use to track and reference your session.\\n\\n```python\\nsession = client.create_fl_session(\\n    name=\"Testing notebook\",\\n    description=\"I am testing session creation through a notebook\",\\n    min_num_clients=2,\\n    num_rounds=2,\\n    package_name=\"iai_ffnet\",\\n    model_config=model_config,\\n    data_config=data_config,\\n).start()\\n\\nsession.id\\n```\\n\\n| TABLE |\\n\\n\\n \\n\\nJoin clients to the session\\n\\nThe next step is to join the session with the sample data. This example has data for two datasets simulating two clients, as specified with the min_num_clients argument. Therefore, to run this example, you will call subprocess.Popen twice to connect each dataset to the session as a separate client. \\n\\nThe session begins training once the minimum number of clients have joined the session.\\n\\nEach client runs as a separate Docker container to simulate distributed data silos.\\n\\nNote: If you extracted the contents of the sample file to a different location than the default, change the data_path in the sample code before attempting to run it.\\n\\n```python\\nimport subprocess\\n\\ndata_path = \"~/Downloads/synthetic\"\\n#Join dataset one (silo0)\\nsubprocess.Popen(f\"iai client train --token {IAI_TOKEN} --session {session.id} --train-path {data_path}/train_silo0.parquet --test-path {data_path}/test.parquet --batch-size 1024 --client-name client-1 --remove-after-complete\",\\n    shell=True\\n)\\n#Join dataset two (silo1)\\nsubprocess.Popen(f\"iai client train --token {IAI_TOKEN} --session {session.id} --train-path {data_path}/train_silo1.parquet --test-path {data_path}/test.parquet --batch-size 1024 --client-name client-2 --remove-after-complete\",\\n    shell=True\\n)\\n```\\n\\nwhere\\n* `data_path` is the path to the sample data on your local machine\\n* `IAI_TOKEN` is your access token \\n* `session.id` is the ID returned by the previous step ()\\n* `train-path` is the path to and name of the sample dataset file\\n\\n \\n\\nPoll for session results\\n\\nDepending on the type of session and the size of the datasets, sessions may take some time to run. In the sample notebook and this tutorial, we poll the server to determine the session status. \\n\\nYou can log information about the session during this time. In this example, we are logging the current round and the clients that have joined the session. \\n\\n```python\\nimport time\\n\\nwhile session.status == \"started\":\\n    session = client.fl_session(id=session.id)\\n    if session.round is None:\\n        print(\"Polling for Session info\")\\n\\n    if session.round is not None:\\n        if session.round > 0:\\n            print(f\"Round: {session.round}\")\\n            print(\"Round Clients\")\\n            print(session.clients)\\n        else:\\n            print(\"Session training clean up\")\\n    print(\"-------------------------\")\\n    time.sleep(1)py\\n```\\n\\nAnother popular option is to log the session.metrics().as_dict() to view the in-progress training metrics.\\n\\n```python\\n{\\'session_id\\': \\'session.id\\',\\n \\'federated_metrics\\': [],\\n \\'rounds\\': [],\\n \\'latest_global_model_federated_loss\\': None}\\n```\\n\\nSession Complete\\n\\nCongratulations, you have your first federated model! You can test it by making predictions. For more information, see .\\n\\n\\n', 'title': 'input/hfl-train-tutorial.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"how do I train a GLM with integrate.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: To begin exploratory data analysis, you must first create a session, the same as you would for training a model. In this case, to configure the session, you must specify either the dataset_config, or num_datasets argument. Using a dataset_config file: The dataset_config file is a configuration file that maps the name of one or more datasets to the columns to be pulled. Dataset names and column names are specified as key-value pairs in the file. For each pair, the keys are dataset names that are expected for the EDA analysis. The values are a list of corresponding columns. The list of columns can be specified as column names (strings), column indices (integers), or a blank list to retrieve all columns from that particular dataset. If a dataset name is not included in the configuration file, all columns from that dataset are used by default.\n",
       "\n",
       "To create an EDA session, we specify a `dataset_config` dictionary indicating the columns to explore for each dataset. Here the empty list `[]` means to include all columns. \n",
       "For information more information on how to configure an EDA session, see the documentation here.\n",
       "\n",
       "```python\n",
       "eda_session"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.33949137', 'text': '\\n\\nStart an EDA Session using IAI client\\nFollow the documentation on directions for how to install the integrate_ai package and the sample data.\\nUnzip the sample data to your `~/Downloads` directory, otherwise update the `data_path` below to point to the sample data.\\n\\n\\n```python\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n",
      "{'score': '0.3567767', 'text': '\\n\\ndata_dir = \\'~/Downloads/synthetic\\'\\nstorage_path = \"azure://test-ron-blob\"\\ntrain_path1 = f\"{data_dir}/train_silo0.parquet\"\\ntrain_path2 = f\"{data_dir}/train_silo1.parquet\"\\ntest_path = f\"{data_dir}/test.parquet\"\\n```\\n\\n \\n\\nCreate and Run EDA Session\\n\\n\\n```python\\ndataset_config = {\"dataset_one\": [], \"dataset_two\": []}\\n\\neda_session = client.create_eda_session(\\n    name=\"Testing notebook - EDA\",\\n    description=\"I am testing EDA session creation through a notebook\",\\n    data_config=dataset_config,\\n    startup_mode=\"external\",\\n).start()\\neda_session.id\\n```\\n\\n\\n```python\\neda_task_group = (\\n    SessionTaskGroup(eda_session)\\n    .add_task(tb.fls(storage_path=storage_path))\\n    .add_task(tb.eda(dataset_name=\"dataset_one\", dataset_path=train_path1))\\n    .add_task(tb.eda(dataset_name=\"dataset_two\", dataset_path=train_path2))\\n)\\n```\\n\\n\\n```python\\neda_task_group_context = eda_task_group.start()\\n```\\n\\n\\n```python\\nwait_and_print(eda_task_group_context)\\n```\\n\\n\\n```python\\nprint(eda_task_group_context.contexts[0].logs())\\n```\\n\\n\\n```python\\nprint(eda_task_group_context.contexts[1].logs())\\n```\\n\\n\\n```python\\nprint(eda_task_group_context.contexts[2].logs())\\n```\\n\\n\\n```python\\nresults = eda_session.results()\\nresults.describe()\\n```\\n\\n', 'title': 'input/integrateai_azure_client_server.md'}\n",
      "{'score': '0.38773423', 'text': '\\n\\nConfigure an EDA Session\\n\\nTo begin exploratory data analysis, you must first create a session, the same as you would for training a model. In this case, to configure the session, you must specify either the dataset_config, or num_datasets argument.\\n\\n \\n\\nUsing a dataset_config file\\n\\nThe dataset_config file is a configuration file that maps the name of one or more datasets to the columns to be pulled. Dataset names and column names are specified as key-value pairs in the file. \\n\\nFor each pair, the keys are dataset names that are expected for the EDA analysis. The values are a list of corresponding columns. The list of columns can be specified as column names (strings), column indices (integers), or a blank list to retrieve all columns from that particular dataset. \\n\\nIf a dataset name is not included in the configuration file, all columns from that dataset are used by default.\\n\\nFor example:\\n\\nTo retrieve all columns for a submitted dataset named dataset_one:\\n`dataset_config = {\"dataset_one\": []}`\\n\\n To retrieve the first column and the column x2 for a submitted dataset named dataset_one:\\n`dataset_config = {\"dataset_one\": [1,\"x2\"]}`\\n\\nTo retrieve the first column and the column x2 for a submitted dataset named dataset_one and all columns in a dataset named dataset_two:\\n`dataset_config = {\"dataset_one\": [1,\"x2\"],\"dataset_two\": []} `\\n\\n', 'title': 'input/hfl-eda.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.0826659', 'text': '\\n\\nConfigure an EDA Session\\n\\nTo begin exploratory data analysis, you must first create a session, the same as you would for training a model. In this case, to configure the session, you must specify either the dataset_config, or num_datasets argument.\\n\\n \\n\\nUsing a dataset_config file\\n\\nThe dataset_config file is a configuration file that maps the name of one or more datasets to the columns to be pulled. Dataset names and column names are specified as key-value pairs in the file. \\n\\nFor each pair, the keys are dataset names that are expected for the EDA analysis. The values are a list of corresponding columns. The list of columns can be specified as column names (strings), column indices (integers), or a blank list to retrieve all columns from that particular dataset. \\n\\nIf a dataset name is not included in the configuration file, all columns from that dataset are used by default.\\n\\nFor example:\\n\\nTo retrieve all columns for a submitted dataset named dataset_one:\\n`dataset_config = {\"dataset_one\": []}`\\n\\n To retrieve the first column and the column x2 for a submitted dataset named dataset_one:\\n`dataset_config = {\"dataset_one\": [1,\"x2\"]}`\\n\\nTo retrieve the first column and the column x2 for a submitted dataset named dataset_one and all columns in a dataset named dataset_two:\\n`dataset_config = {\"dataset_one\": [1,\"x2\"],\"dataset_two\": []} `\\n\\n', 'title': 'input/hfl-eda.md'}\n",
      "{'score': '0.25490403', 'text': '\\n\\nSpecifying the number of datasets\\n\\nYou can manually set the number of datasets that are expected to be submitted for an EDA session by specifying a value for `num_datasets`. \\n\\nIf `num_datasets` is not specified, the number is inferred from the number of datasets provided in dataset_config.\\n\\n \\n\\nCreate and start an EDA session\\n\\nThe following code sample demonstrates creating and starting an EDA session to perform privacy-preserving data analysis on two datasets, named dataset_one and dataset_two. It returns an EDA session ID that you can use to track and reference your session.\\n\\nThe dataset config used here specifies that the first column (x1), x5, and x7 will be analyzed for dataset_one and columns x1, x10, and x11 will be analyzed for dataset_two.\\n\\nSince the num_datasets argument is not provided to client.create_eda_session(), the number of datasets is inferred as two from the dataset_config.\\n\\n`dataset_config = {\"dataset_one\": [1,\"x5\",\"x7\"], \"dataset_two\": [\"x1\",\"x10\",\"x11\"]}`\\n\\n```python\\neda_session = client.create_eda_session(\\n    name=\"Testing notebook - EDA\",\\n    description=\"I am testing EDA session creation through a notebook\",\\n    data_config=dataset_config\\n).start()\\n\\neda_session.id\\n```\\n\\nFor more information, see the `create_eda_session()` definition in the API documentation.\\n\\n', 'title': 'input/hfl-eda.md'}\n",
      "{'score': '0.25804642', 'text': '\\n\\nSet your AWS Credentials if you are generating temporary ones, else use the default profile credentials\\naws_creds = {\\n    \"ACCESS_KEY\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\\n    \"SECRET_KEY\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\\n    \"SESSION_TOKEN\": os.environ.get(\"AWS_SESSION_TOKEN\"),\\n    \"REGION\": os.environ.get(\"AWS_REGION\"),\\n}\\n```\\n\\n \\n\\nCreate an EDA Session for exploring the datasets\\n\\nTo create an EDA session, we specify a `dataset_config` dictionary indicating the columns to explore for each dataset. Here the empty list `[]` means to include all columns. \\n\\nFor information more information on how to configure an EDA session, see the documentation here.\\n\\n\\n```python\\neda_data_config = {\"prl_silo0\": [], \"prl_silo1\": []}\\neda_config = {\"strategy\": {\"name\": \"EDAHistogram\", \"params\": {}}}\\nprl_session_id = \"\"\\n```\\n\\n\\n```python\\neda_session = client.create_eda_session(\\n    name=\"Testing notebook - EDA Intersect session\",\\n    description=\"I am testing EDA on PRL session creation through a notebook\",\\n    data_config=eda_data_config,\\n    eda_mode=\"intersect\",\\n    prl_session_id=prl_session_id,\\n).start()\\n\\neda_session.id\\n```\\n\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"how to create an EDA session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: For this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\n",
       "\n",
       "```python\n",
       "prl_data_config = {\n",
       "    \"clients\": {\n",
       "        \"passive_client\": {\"id_columns\": [\"id\"]},\n",
       "        \"active_client\": {\"id_columns\": [\"id\"]},\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.36473554', 'text': '\\n\\nSample PRL Data Config\\n\\nFor this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\\n\\n\\n```python\\nprl_data_config = {\\n    \"clients\": {\\n        \"passive_client\": {\"id_columns\": [\"id\"]},\\n        \"active_client\": {\"id_columns\": [\"id\"]},\\n    }\\n}\\n```\\n\\n', 'title': 'input/integrateai_fargate_batch_client_vfl.md'}\n",
      "{'score': '0.37615478', 'text': '\\n\\nPRL Data Config\\n\\nFor this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\\n\\n\\n```python\\nprl_data_config = {\\n    \"clients\": {\\n        \"active_client\": {\"id_columns\": [\"id\"]},\\n        \"passive_client\": {\"id_columns\": [\"id\"]},\\n    }\\n}\\n```\\n\\n', 'title': 'input/integrateai_batch_client_vfl.md'}\n",
      "{'score': '0.38360363', 'text': '\\n\\nPRL Session Overview\\n\\nIn PRL, two parties submit paths to their datasets so that they can be aligned to perform a machine learning task.\\n1. ID columns (id_columns) are used to produce a hash that is sent to the server for comparison.\\nThe secret for this hash is shared between the clients and the server has no knowledge of it. This comparison is the Private Set Intersection (PSI) part of PRL.\\n2. Once compared, the server orchestrates the data alignment because it knows which indices of each dataset are in common. This is the Private Record Alignment (PRA) part of PRL. \\n\\nFor information about privacy when performing PRL, see PRL Privacy for VFL.\\n\\n', 'title': 'input/prl-session.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.06930387', 'text': '\\n\\nPRL Data Config\\n\\nFor this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\\n\\n\\n```python\\nprl_data_config = {\\n    \"clients\": {\\n        \"active_client\": {\"id_columns\": [\"id\"]},\\n        \"passive_client\": {\"id_columns\": [\"id\"]},\\n    }\\n}\\n```\\n\\n', 'title': 'input/integrateai_batch_client_vfl.md'}\n",
      "{'score': '0.07922597', 'text': '\\n\\nSample PRL Data Config\\n\\nFor this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\\n\\n\\n```python\\nprl_data_config = {\\n    \"clients\": {\\n        \"passive_client\": {\"id_columns\": [\"id\"]},\\n        \"active_client\": {\"id_columns\": [\"id\"]},\\n    }\\n}\\n```\\n\\n', 'title': 'input/integrateai_fargate_batch_client_vfl.md'}\n",
      "{'score': '0.12989154', 'text': '\\n\\nCreate a configuration for the PRL session.\\n\\nSpecify a `prl_data_config` that indicates the columns to use as identifiers when linking the datasets to each other. The number of items in the config specifies the number of expected clients. In this example, there are two items and therefore two clients submitting data. Their datasets are linked by the \"id\" column in any provided datasets.\\n\\n```python\\nprl_data_config = {\\n    \"clients\": {\\n        \"passive_client\": {\"id_columns\": [\"id\"]},\\n        \"active_client\": {\"id_columns\": [\"id\"],},\\n    }\\n}\\n```\\n\\n', 'title': 'input/prl-session.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"give me an example of data config for PRL sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: Currently, the integrate.ai HFL model supports the following strategies: \n",
       "\n",
       "- Federated Averaging (FedAvg) \n",
       "- Federated Stochastic Gradient Descent (FedSGD) \n",
       "- Federated Proximal Stochastic Gradient Descent (FedProxSGD) \n",
       "- Federated Momentum (FedMomentum) \n",
       "- Federated Adagrad (FedAdagrad) \n",
       "- Federated Adam (FedAdam) \n",
       "- Federated Proximal Adam (FedProxAdam) \n",
       "- Federated Averaged Stochastic Gradient Descent (FedAvgSGD) \n",
       "- Federated Averaged Proximal Stochastic Gradient Descent (FedAvgProxSGD) \n",
       "- Federated Averaged Momentum (FedAvgMomentum) \n",
       "- Federated Averaged Adagrad (FedAvgAdagrad) \n",
       "- Federated Averaged Adam (FedAvgAdam) \n",
       "- Federated Averaged Proximal Adam (FedAvgProxAdam) \n",
       "- Federated Stochastic Gradient Descent with Momentum (FedSG"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.38951057', 'text': '\\n\\nFFNet\\n\\n ## HFL\\n\\n ## VFL \\n\\n\\n\\n', 'title': 'input/train-overview.md'}\n",
      "{'score': '0.42059958', 'text': '\\n\\nHFL FFNet\\n\\nThe iai_ffnet model is a feedforward neural network for horizontal federated learning (HFL) that uses the same activation for each hidden layer.\\n\\nThis model only supports classification and regression. Custom loss functions are not supported. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.4634987', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.32290506', 'text': '\\n\\nHFL FFNet\\n\\nThe iai_ffnet model is a feedforward neural network for horizontal federated learning (HFL) that uses the same activation for each hidden layer.\\n\\nThis model only supports classification and regression. Custom loss functions are not supported. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.34372133', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "{'score': '0.37342918', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"which strategies are currently supported for HFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: A VFL session is a session in the integrate.ai platform that allows you to link two datasets together. This is done by creating a session that contains the two datasets, and then using the session ID to link the datasets together. The session can then be used to train a model or to make predictions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.43872964', 'text': '\\n\\nintegrate.ai VFL Flow\\n\\nThe following diagram outlines the training flow in the integrate.ai implementation of VFL.\\n\\n\\n', 'title': 'input/vfl-train.md'}\n",
      "{'score': '0.4483566', 'text': '\\n\\nFFNet\\n\\n ## HFL\\n\\n ## VFL \\n\\n\\n\\n', 'title': 'input/train-overview.md'}\n",
      "{'score': '0.5002715', 'text': '\\n\\nSession Complete!\\nNow you can view the vfl training metrics and start making predictions\\n\\n\\n```python\\nvfl_train_session.metrics().as_dict()\\n```\\n\\n\\n```python\\nfig = vfl_train_session.metrics().plot()\\n```\\n\\n \\n\\nMake a Prediction on the trained VFL Model\\nTo create a VFL predict session, specify the `prl_session_id` indicating the session above used to link the datasets together. You also need the `training_id` of the above VFL train session.The `vfl_mode` needs to be set to `\\'predict\\'`.\\n\\n\\n```python\\nvfl_predict_session = client.create_vfl_session(\\n    name=\"Testing notebook - VFL Predict\",\\n    description=\"I am testing VFL Predict session creation through a notebook\",\\n    prl_session_id=prl_session.id,\\n    training_session_id=vfl_train_session.id,\\n    vfl_mode=\\'predict\\',\\n    data_config=data_config\\n).start()\\n\\nvfl_predict_session.id\\n```\\n\\n \\n\\nSpecify the full path to store your predictions including file name\\n\\n\\n```python\\nactive_predictions_storage_path=\"{full path of predictions file name}\"\\nvfl_predict_task_group_context = SessionTaskGroup(vfl_predict_session)\\\\\\n        .add_task(task_server.fls(subnet_id, security_group, storage_path=model_storage, client=client))\\\\\\n        .add_task(tb.vfl_predict(client_name=\\'active_client\\', dataset_path=test_path2, vcpus=\\'2\\', memory=\\'16384\\', batch_size=1024, storage_path=active_predictions_storage_path, client=client, raw_output=True))\\\\\\n        .add_task(tb.vfl_predict(client_name=\\'passive_client\\', dataset_path=test_path1, vcpus=\\'2\\', memory=\\'16384\\', batch_size=1024, storage_path=\"None\", client=client, raw_output=True)).start()\\n```\\n\\n', 'title': 'input/integrateai_fargate_batch_client_vfl.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.29408303', 'text': '\\n\\nintegrate.ai VFL Flow\\n\\nThe following diagram outlines the training flow in the integrate.ai implementation of VFL.\\n\\n\\n', 'title': 'input/vfl-train.md'}\n",
      "{'score': '0.30614954', 'text': '\\n\\nVFL SplitNN\\n\\nintegrate.ai also supports the SplitNN model for vertical federated learning (VFL). In this model, neural networks are trained with data across multiple clients. A PRL (private-record linking) session is required for all datasets involved. There are two types of sessions: train, and predict. To make predictions, the PRL session ID and the corresponding training session ID are required. \\n\\nFor more information, see  and .\\n\\n \\n\\nGeneralized Linear Models (GLMs)\\n\\nThis model class supports a variety of regression models. Examples include linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. We also support regularizing the model coefficients with the elastic net penalty.\\n\\nExamples of use cases include [1]:\\n\\n* Agriculture / weather modeling: number of rain events per year, amount of rainfall per event, total rainfall per year\\n* Risk modeling / insurance policy pricing: number of claim events / policyholder per year, cost per event, total cost per policyholder per year\\n* Predictive maintenance: number of production interruption events per year, duration of interruption, total interruption time per year\\n\\nThe iai_glm model trains generalized linear models by treating them as a special case of single-layer neural nets with particular output activation functions. \\n\\n', 'title': 'input/iai_ffnet.md'}\n",
      "{'score': '0.3406458', 'text': \"\\n\\nAPI and SDK\\n\\n\\n\\n\\n\\n \\n\\nPRL Privacy\\n\\nIn vertical federated learning (VFL), the datasets shared by any two parties must have some overlap and alignment in order to be used for machine learning tasks. There are typically two main problems:\\nThe identifiers between the two datasets are not fully overlapped.\\nThe rows of the filtered, overlapped records for the datasets are not in the same order.\\nTo resolve these differences while maintaining privacy, integrate.ai applies private record linkage (PRL), which consists of two steps: determining the overlap (or intersection) and aligning the rows.\\nPrivate Set Intersection\\nFirst, Private Set Intersection (PSI) is used to determine the overlap without storing the raw data centrally, or exposing it to any party. PSI is a privacy-preserving technique that is considered a Secure multiparty computation (SMPC) technology. This type of technology uses cryptographic techniques to enable multiple parties to perform operations on disparate datasets without revealing the underlying data to any party. \\nAdditionally, integrate.ai does not store the raw data on a server. Instead, the parties submit the paths to their datasets. integrate.ai uses the ID columns of the datasets to produce a hash locally that is sent to the server for comparison. The secret for this hash is shared only through Diffie–Hellman key exchange between the clients - the server has no knowledge of it. \\nPrivate Record Alignment\\nOnce the ID columns are compared, the server knows which dataset indices are common between the two sets and can align the rows. This step is the private record alignment portion of PRL. It enables machine learning tasks to be performed on the datasets. \\nIn VFL, match indices are sent to both parties. This means there is a possibility that a user within one of the parties could potentially learn the intersection if they reverse engineer the fit instructions. This risk is mitigated by system-level access control. \\nFor more information about running PRL sessions, see Private Record Linkage sessions.\\n\\n\\n\\n \\n\\nPrivate Record Linkage (PRL) sessions\\n\\nPrivate record linkage sessions create intersection and alignment among datasets to prepare them for vertical federated learning.\\n\\nIn a vertical federated learning process, two or more parties collaboratively train a model using datasets that share a set of overlapping features. These datasets generally each contain distinct data with some overlap. This overlap is used to define the intersection of the sets. Private record linkage (PRL) uses the intersection to create alignment between the sets so that a shared model can be trained. \\n\\nOverlapping records are determined privately through a PRL session, which combines Private Set Intersection with Private Record Alignment.\\n\\nFor example, in data sharing between a hospital (party B, the Active party) and a medical imaging centre (party A, the Passive party), only a subset of the hospital patients will exist in the imaging centre's data. The hospital can run a PRL session to determine the target subset for model training. \\n\\n\\n\", 'title': 'input/overview.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"what is a VFL session\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: Pandas is an open source library for data analysis and manipulation in Python. It provides data structures and operations for manipulating numerical tables and time series. It is built on top of the NumPy library and is designed for working with tabular data. Pandas provides powerful data analysis tools such as data filtering, aggregation, and visualization. It also provides support for missing data, time series analysis, and statistical modeling."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.60430247', 'text': '\\n\\nClass name: TestDataset\\n\\nFunctions: \\ntest_pandas_dataset_encode, test_hash_dataframe, \\nDocumentation: \\n\\n\\nClass TestDataset\\n\\nThis class provides methods to test a pandas dataset for encoding and hashing.\\n\\nMethods:\\n\\ntest_pandas_dataset_encode(dataset):\\n    This method tests a pandas dataset for encoding. It checks if the dataset is encoded correctly and if the data types are correct.\\n\\ntest_hash_dataframe(dataset):\\n    This method tests a pandas dataset for hashing. It checks if the data is hashed correctly and if the data types are correct.\\n', 'title': 'input/tests/test_dataset.md'}\n",
      "{'score': '0.6137383', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(\"./test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.6157502', 'text': \"\\n\\npoll for status\\nvfl_predict_task_group_context.wait(300)\\nView VFL Predictions\\nOnce the predict session completes successfully, you can view the predictions from the Active party and evaluate the performance.\\n\\nimport pandas as pd\\n\\ndf_pred = pd.read_csv(active_predictions_storage_path)\\ndf_pred.head()\\nExample output:\\n\\n\\n \\n\\nVFL SplitNN Model Training\\n\\nIn a vertical federated learning (VFL) process, two or more parties collaboratively train a model using datasets that share a set of overlapping features. Each party has partial information about the overlapped subjects in the dataset. Therefore, before running a VFL training session, a private record linkage (PRL) session is performed to find the intersection and create alignment between datasets. \\n\\nThere are two types of parties participating in the training:\\n* The Active Party owns the labels, and may or may not also contribute data.\\n* The Passive Party contributes only data. \\n\\nFor example, in data sharing between a hospital (party B, the Active party) and a medical imaging centre (party A, the Passive party), only a subset of the hospital patients will exist in the imaging centre's data. The hospital can run a PRL session to determine the target subset for VFL model training.\\n\\n\", 'title': 'input/vfl-predict.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.47228232', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n",
      "{'score': '0.47261146', 'text': '\\n\\nRemote datasets\\n\\nOne of the key features of the integrate.ai platform is the ability to work with datasets without having to colocate them. \\nThe integrate.ai client and SDK are capable of working with datasets that are hosted remotely on AWS S3. You must set up and configure the AWS CLI to use S3 datasets.\\n\\n', 'title': 'input/data-requirements.md'}\n",
      "{'score': '0.4765334', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(\"./test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"what is pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: You can use the `pandas` library to read parquet files. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(\"./test.parquet\")\n",
       "```\n",
       "\n",
       "If you need to specify a path to the parquet file, you can use the `f-string` format to do so. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(f\"{data_path}/test.parquet\")\n",
       "```\n",
       "\n",
       "If you have already defined the path to the parquet file, you can use the variable name instead. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(test_path)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3535117', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(\"./test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.37380177', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.37380177', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.20677531', 'text': '\\n\\nLoad the test data\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\nExample (image)\\n\\n', 'title': 'input/hfl-gbm.md'}\n",
      "{'score': '0.21050414', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.21050414', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"how to load parquet files?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: You can use the `pd.read_parquet()` function to read parquet files directly with pandas. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(\"./test.parquet\")\n",
       "```\n",
       "\n",
       "You can also use the `f-string` format to specify the path to the parquet file. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(f\"{data_path}/test.parquet\")\n",
       "```\n",
       "\n",
       "If you have the path to the parquet file stored in a variable, you can use that as well. \n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "test_data = pd.read_parquet(test_path)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3372351', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(\"./test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_batch_client.md'}\n",
      "{'score': '0.36065042', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.36065042', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.22612014', 'text': '\\n\\nLoad the test data\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\nExample (image)\\n\\n', 'title': 'input/hfl-gbm.md'}\n",
      "{'score': '0.2266917', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.2266917', 'text': '\\n\\nLoad test data\\n\\n\\n```python\\nimport pandas as pd\\n\\ntest_data = pd.read_parquet(f\"{data_path}/test.parquet\")\\ntest_data.head()\\n```\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"how to load parquet files directly with pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: We can compute the moving average of all columns in a dataframe using the pandas.DataFrame.rolling() method. For example, the following code computes the moving average of all columns in a dataframe with a window size of 3:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Create a dataframe\n",
       "df = pd.DataFrame({'A': [1, 2, 3, 4, 5],\n",
       "                   'B': [2, 3, 4, 5, 6],\n",
       "                   'C': [3, 4, 5, 6, 7]})\n",
       "\n",
       "# Compute the moving average\n",
       "df.rolling(window=3).mean()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.5283706', 'text': '\\n\\nintegrate.ai API LSTM Sample Notebook\\n\\n', 'title': 'input/integrateai_custom_lstm.md'}\n",
      "{'score': '0.531085', 'text': '\\n\\nintegrate.ai HFL Gradient Boosting Methods Sample Notebook\\n\\n', 'title': 'input/integrateai_api_gbm.md'}\n",
      "{'score': '0.53433895', 'text': '\\n\\nGLM\\n\\n#GBM\\n\\n#LSTM\\n\\n', 'title': 'input/train-overview.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.44940227', 'text': '\\n\\nFunction name: mean\\n\\nFunction: \\n```\\ndef mean(self) -> float:\\n        \"\"\"Calculates the mean of the column data.\\n        Will return NaN if column is not continuous.\\n        Returns:\\n            Mean of column data.\\n        \"\"\"\\n        if not self.is_continuous:\\n            return np.nan\\n        return (self.counts * self._bin_avg).sum() / self._count\\n```, \\nDocumentation: \\n\\nThis function calculates the mean of the column data. It will return NaN if the column is not continuous. It takes in the column data as an argument and returns the mean of the column data as a float.\\n\\n \\n\\nFunction name: percentile\\n\\nFunction: \\n```\\ndef percentile(self, p: float) -> float:\\n        \"\"\"Calculates percentile p of the column data.\\n        Will return NaN if column is not continuous.\\n        Args:\\n            p (float): a number between 0 and 1\\n        Returns:\\n            Percentile of the column data.\\n        \"\"\"\\n        if not self.is_continuous:\\n            return np.nan\\n\\n        if not isinstance(p, float) or p = 1:\\n            raise IaiPythonAPIException(f\"p should be a value between 0(inclusive) and 1(inclusive), but {p} is given.\")\\n\\n        point = self._count * p\\n        for i in range(len(self.counts)):\\n            point -= self.counts[i]\\n            if point <= 0:\\n                return self.bins[i] + (self.bins[i + 1] - self.bins[i]) * (abs(point) / self.counts[i])\\n        raise IaiPythonAPIException(\"U r not supposed to reach this line of code!\")\\n```, \\nDocumentation: \\n\\nThis function calculates the percentile p of the column data. It will return NaN if the column is not continuous. The function takes in a float value between 0 and 1 as an argument and returns the percentile of the column data. If the argument is not a float or is not between 0 and 1, an exception is raised.\\n\\n', 'title': 'input/src/integrate_ai_sdk/api/eda.md'}\n",
      "{'score': '0.47780365', 'text': '\\n\\nFunction name: median\\n\\nFunction: \\n```\\ndef median(self) -> float:\\n        \"\"\"Calculates the median of the column data.\\n        Will return NaN if column is not continuous.\\n        Returns:\\n            Median of the column data.\\n        \"\"\"\\n        if not self.is_continuous:\\n            return np.nan\\n\\n        return self.percentile(p=0.5)\\n```, \\nDocumentation: \\n\\nThis function calculates the median of the column data. It will return NaN if the column is not continuous. It takes no parameters and returns the median of the column data as a float.\\n\\n', 'title': 'input/src/integrate_ai_sdk/api/eda.md'}\n",
      "{'score': '0.47862327', 'text': '\\n\\nFunction name: var\\n\\nFunction: \\n```\\ndef var(self) -> float:\\n        \"\"\"Calculates the variance of the column data.\\n        Will return NaN if column is not continuous.\\n        Returns:\\n            Variance of the column data.\\n        \"\"\"\\n        if not self.is_continuous:\\n            return np.nan\\n\\n        ms = self.mean()\\n        return (self.counts * (self._bin_avg - ms) ** 2).sum() / self._count\\n```, \\nDocumentation: \\n\\nThis function calculates the variance of the column data. It will return NaN if the column is not continuous. It takes in the column data as an argument and returns the variance of the column data.\\n\\n', 'title': 'input/src/integrate_ai_sdk/api/eda.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"gimme an example of computing the moving average of all columns in a dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Integrate.ai uses the following Privacy Enhancing Technologies (PETs) to protect user data: \n",
       "\n",
       "- Data Masking: Data masking is used to protect sensitive data by replacing it with a non-sensitive version. This is done by replacing the original data with a randomly generated value that is consistent across all users. \n",
       "\n",
       "- Data Anonymization: Data anonymization is used to protect user data by removing personally identifiable information (PII) from the data. This is done by removing or replacing any data that could be used to identify an individual. \n",
       "\n",
       "- Data Encryption: Data encryption is used to protect user data by encrypting it so that it can only be accessed by authorized users. This is done by using a secure encryption algorithm to scramble the data so that it is unreadable to anyone without the encryption key. \n",
       "\n",
       "- Access Control: Access control is used to protect user data by restricting access to only authorized users. This is done by setting up user roles and permissions that determine who can access what data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.34474123', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.34474123', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n",
      "{'score': '0.37805355', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.37125948', 'text': \"\\n\\nCustom Models\\n\\n\\n\\n\\n \\n\\nUser Authentication\\n\\nSharing access to training sessions and shared models in a simple and secure manner is a key requirement for many data custodians. integrate.ai provides a secure method of authenticating end users with limited permissions through the SDK to enable privileged access. \\n\\nAs the user responsible for managing access through the integrate.ai platform, you have the ability to generate an **unscoped API token** through the integrate.ai UI. Unscoped API tokens provide full access to the integrate.ai SDK. You can run client training tasks locally, or on remote data. \\n\\nIn the case that you want to create a token that has limited access, to enforce governance standards or provide an end user of your platform with limited access to the integrate.ai SDK, you can create **scoped API tokens**. Scoped tokens grant limited permissions, which enables you to control the level of access to trained sessions and models.\\n\\nIn the UI, you can view your personal API tokens as well as all scoped API tokens created in your organization's workspace through the SDK. These scoped user tokens are designed for use with the integrate.ai SDK. Tokens are tied to user identities through a unique ID, which is logged with each user action.\\n\\nLimiting user access by token greatly reduces the security risk of leaked credentials. For example, with an unscoped API token, a user could run tasks on a remote machine, where there is a risk that it could be leaked or exposed because it is shared in an outside (non-local) environment. To mitigate that risk, you can instead provide the user with a scoped token that has limited permissions and a short lifespan (maximum 30 days).\\n\\n \\n\\nCreate an unscoped token\\n\\nAs the user who manages other users' acce\\nss, you must first create your own unscoped token.\\n\\n1. Log in to your integrate.ai account on the web. \\n2. On the Dashboard, click Generate Access Token.\\n3. Copy the access token and save it to a secure location. \\n\\nThis is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\n\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as .\\n\\n\", 'title': 'input/train-overview.md'}\n",
      "{'score': '0.39990398', 'text': '\\n\\nRemote datasets\\n\\nOne of the key features of the integrate.ai platform is the ability to work with datasets without having to colocate them. \\nThe integrate.ai client and SDK are capable of working with datasets that are hosted remotely on AWS S3. You must set up and configure the AWS CLI to use S3 datasets.\\n\\n', 'title': 'input/data-requirements.md'}\n",
      "{'score': '0.40244123', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"what PETs is Integrate.ai using?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: I'm sorry, I'm not able to answer that question. However, I can tell you that Integrate.ai is a leading AI platform that helps companies build and deploy AI solutions quickly and easily. It provides a suite of tools and services to help companies create, deploy, and manage AI models. It also provides access to a wide range of data sources and services to help companies build and deploy AI models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'score': '0.3499709', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n",
      "{'score': '0.3699282', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_api.md'}\n",
      "{'score': '0.3699282', 'text': '\\n\\nintegrate.ai API Sample Notebook\\n\\n', 'title': 'input/integrateai_eda_intersect_batch.md'}\n",
      "-------------------------------------\n",
      "{'score': '0.28809512', 'text': '\\n\\nPlatform Overview\\n\\nThe integrate.ai SaaS platform consists of 3 main components:\\n1. A federated learning server and backend.\\n2. A web app for workspace administration.\\n3. A robust API and SDK that support federated analytical orchestration.\\n\\n', 'title': 'input/overview.md'}\n",
      "{'score': '0.3516042', 'text': '\\n\\nRemote datasets\\n\\nOne of the key features of the integrate.ai platform is the ability to work with datasets without having to colocate them. \\nThe integrate.ai client and SDK are capable of working with datasets that are hosted remotely on AWS S3. You must set up and configure the AWS CLI to use S3 datasets.\\n\\n', 'title': 'input/data-requirements.md'}\n",
      "{'score': '0.3606072', 'text': '\\n\\nInstall integrate.ai packages\\n\\n1. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: \\n`pip install integrate-ai`\\n2. Install the SDK package using the access token you generated.\\n`iai sdk install --token `\\n3. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container.\\n`iai client pull --token `\\n\\n*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment.\\n\\n\\n', 'title': 'input/install-sdk.md'}\n"
     ]
    }
   ],
   "source": [
    "query(\"why Integrate.ai is the best company in the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iai_local_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
