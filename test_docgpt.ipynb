{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"application/prompts/history_questions.txt\", \"r\") as f:\n",
    "    history_questions = f.readlines()\n",
    "\n",
    "with open(\"application/prompts/history_answers.txt\", \"r\") as f:\n",
    "    history_answers = f.readlines()\n",
    "\n",
    "hist = []\n",
    "for i in range(len(history_questions)):\n",
    "    hist.append({\"prompt\": history_questions[i].strip(), \"response\": history_answers[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'How do I get started?',\n",
       "  'response': '# Install integrate.ai SDK and Client\\\\n\\\\n## Generate an access token\\\\n\\\\nTo install the client and SDK, you must generate an access token through the web portal. \\\\n1. Log in to your workspace through the portal.\\\\n2. On the Dashboard, click Generate Access Token.\\\\n3. Copy the acccess token and save it to a secure location.\\\\n\\\\nImportant: This is the only time that the API token can be viewed or downloaded. If you lose or forget your API token, you cannot retrieve it. Instead, create a new API token and revoke the old one. You can manage API tokens through the web portal. \\\\n\\\\nTreat your API tokens like passwords and keep them secret. When working with the API, use the token as an environment variable instead of hardcoding it into your programs. In this documentation, the token is referenced as `<IAI_TOKEN>`.\\\\n\\\\n## Download the sample notebook\\\\n\\\\n\\\\n## Install integrate.ai packages\\\\n\\\\n1. At a command prompt on your machine, run the following command to install the management tool for the SDK and client: \\\\n`pip install integrate-ai`\\\\n2. Install the SDK package using the access token you generated.\\\\n`iai sdk install --token <IAI_TOKEN>`\\\\n3. Install the integrate.ai client using the same access token. The client is a Docker image that runs in a container.\\\\n`iai client pull --token <IAI_TOKEN>`\\\\n\\\\n*Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment.\\n'},\n",
       " {'prompt': 'How do I use a custom model?',\n",
       "  'response': 'Using the template files provided, create a custom model package. \\\\n\\\\nFollow the naming convention for files in the custom package: no spaces, no special characters, no hyphens, all lowercase characters.\\\\n\\\\n1. Create a folder to contain your custom model package. For this tutorial, this folder is named myCustomModel, and is located in the same parent folder as the template folder.\\\\n\\\\nExample path: C:\\\\<workspace>\\\\integrate_ai_sdk\\\\sample_packages\\\\myCustomModel\\\\n\\\\n2. Create two files in the custom model package folder: \\\\n    a. `model.py` - the custom model definition. You can rename the template_model.py as a starting point for this file.\\\\n    b. `<model-class-name>.json` - default model inputs for this model. It must have the same name as the model class name that is defined in the model.py file. \\\\n\\\\nIf you are using the template files, the default name is `templatemodel.json`.\\\\n\\\\n3. _Optional_: To use a custom dataloader, you must also create a `dataset.py` and a dataset configuration JSON file in the same folder. For more information, see [#](Creating a Custom Dataloader).\\\\n\\\\nIf there is no custom dataset file, the default `TabularDataset` loader is used. It loads .parquet  and .csv files, and requires predictors: [\"x1\", \"x2\"], target: y as input for the data configuration. This is what is used for the standard models.\\\\n\\\\nThe example below provides the boilerplate for your custom model definition. Fill in the code required to define your model. Refer to the model.py files provided for the lstmTagger and cifar10_vgg16 examples if needed. \\\\n\\\\n```python\\\\nfrom integrate_ai_sdk.base_class import IaiBaseModule\\\\n\\\\nclass TemplateModel(IaiBaseModule):\\\\n    def __init__(self):\\\\n        \"\"\"\\\\n        Here you should instantiate your model layers based on the configs.\\\\n        \"\"\"\\\\n        super(TemplateModel, self).__init__()\\\\n\\\\n    def forward(self):\\\\n        \"\"\"\\\\n        The forward path of a model. Can take an input tensor and return a prediction tensor\\\\n        \"\"\"\\\\n        pass\\\\n\\\\nif __name__ == \"__main__\":\\\\n    template_model = TemplateModel()\\\\n```\\n'},\n",
       " {'prompt': 'How do I generate a non-admin token?',\n",
       "  'response': 'Create a scoped token for the user. Include only the scopes that the user requires to work with the system and their data. \\\\n\\\\n```python\\\\ntoken = auth_client.create_token(user_id=user_name, scopes=[Scope.create_session, Scope.read_user_session])\\\\nprint(token)p\\\\n```\\\\n\\\\nThis request returns the unique user ID (the generated email), a list of the granted scopes, and the token, as well as the token ID and the user name. \\\\n\\\\nCopy and save the token somewhere secure to share with the user. '}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query(question, url=\"http://0.0.0.0:7091/api/answer\", history=hist):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"history\": history,\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"embeddings_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    "\n",
    "    res = requests.post(url=url, data=json.dumps(payload), headers=headers)\n",
    "    display(Markdown(res.json()[\"answer\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAI DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To get started, you will need to install the integrate.ai SDK and Client. \n",
       "\n",
       "1. Generate an access token through the web portal. \n",
       "2. Download the sample notebook. \n",
       "3. Install the integrate.ai packages using the access token. \n",
       "4. *Optional*: If you are building a model for data that includes images or video, follow the steps below for Setting up a Docker GPU Environment. \n",
       "5. Create a custom model package. \n",
       "6. Train the model. \n",
       "7. Deploy the model. \n",
       "8. Test the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"How do I get started?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To use a custom model, you must first create a custom model package. This package contains the model class, the model configuration, and the data configuration. \n",
       "\n",
       "1. Choose a name for your custom model, and set the path for the model and data configurations. Note that the name for your custom model **must be unique**. This means that the name for your custom model cannot already be in the Package Name column of the Custom Models Packages Table in the Model Library Page of the UI.\n",
       "\n",
       "2. Create a model class that inherits from the Model class. This class should contain the model architecture and the methods for training, evaluating, and predicting. \n",
       "\n",
       "```python\n",
       "from integrate_ai_sdk.sample_packages.lstmTagger.model import LSTMTagger\n",
       "\n",
       "model = LSTMTagger(embedding_dim=4, hidden_dim=3, output_size=4, vocab_size=9)\n",
       "```\n",
       "\n",
       "3. Create a model configuration file that contains the model parameters and hyperparameters. \n",
       "\n",
       "4. Create a data configuration file that contains the data paths and data preprocessing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"How do I use a custom model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To generate a non-admin token, you must first create a user in the workspace. \n",
       "\n",
       "1. Log in to your workspace through the portal.\n",
       "2. On the Dashboard, click Users.\n",
       "3. Click Add User.\n",
       "4. Enter the user's email address and select the appropriate role.\n",
       "5. Click Generate Access Token.\n",
       "6. Copy the access token and save it to a secure location.\n",
       "\n",
       "The user can now use the token to access the workspace."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"how do I generate a non-admin token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To renew your token, create a new API token and revoke the old one. You can manage API tokens through the web portal. \n",
       "\n",
       "1. Log in to your workspace through the portal.\n",
       "2. On the Dashboard, click Generate Access Token.\n",
       "3. Copy the acccess token and save it to a secure location.\n",
       "4. Revoke the old token.\n",
       "5. Use the new token for all future requests."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"How do I renew my token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Differential privacy parameters can be specified during session creation, within the model configuration. \n",
       "\n",
       "The following code example shows how to set the differential privacy parameter for a session: \n",
       "\n",
       "```python\n",
       "session_config = SessionConfig(name=\"My Session\",\n",
       "                               differential_privacy_parameters=DifferentialPrivacyParameters(epsilon=0.1))\n",
       "```\n",
       "\n",
       "The `epsilon` parameter is the privacy budget, which is the maximum amount of privacy loss that is allowed. The higher the value, the less privacy is preserved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"How do I set my differential privacy parameter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: This error indicates that there was an error in the service handler. This could be due to a misconfiguration of the service, or an issue with the network connection. Check the network connection and the service configuration to ensure that everything is set up correctly. If the issue persists, contact your system administrator for assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"\"\"What does this error mean:\n",
    "```2023-05-24 02:19:25,244 FLOUR MainThread ERROR | neural_net.py:215 | <_MultiThreadedRendezvous of RPC that terminated with:\n",
    "\tstatus = StatusCode.UNKNOWN\n",
    "\tdetails = \"Error in service handler!\"\n",
    "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:99.79.32.55:9999 {grpc_message:\"Error in service handler!\", grpc_status:2, created_time:\"2023-05-24T02:19:25.193640106+00:00\"}\"\n",
    ">\n",
    "05/24/2023 02:19:25:ERROR:<_MultiThreadedRendezvous of RPC that terminated with:\n",
    "\tstatus = StatusCode.UNKNOWN\n",
    "\tdetails = \"Error in service handler!\"\n",
    "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:99.79.32.55:9999 {grpc_message:\"Error in service handler!\", grpc_status:2, created_time:\"2023-05-24T02:19:25.193640106+00:00\"}\"\n",
    ">\n",
    "CLI0006: Unable to join server for Session 'dd5a90b700'```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AI: This error indicates that the AWS Access Key ID you provided is invalid or does not exist in the AWS records. Please check that the Access Key ID is correct and that it is associated with the correct AWS account."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"\"\"What does this error mean:\n",
    "```An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: The best way to get the model weights from an HFL logistic regression analysis is to use the `get_weights()` method. This method returns a dictionary containing the weights for each feature in the model. \n",
       "\n",
       "```python\n",
       "weights = model.get_weights()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"What's the best way to get the model weights from an HFL logistic regression analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To set up multi-client training, you will need to split your data into two silos, one for each client. Each silo should contain the features that the client will use for training. For example, if one client has Y, X1 and the other has X2, the first silo should contain Y and X1, and the second silo should contain X2. \n",
       "\n",
       "You can then use the following commands to set up the clients for training: \n",
       "```python\n",
       "train_path1 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo0.parquet\"\n",
       "train_path2 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo1.parquet\"\n",
       "\n",
       "client_1 = subprocess.Popen(\n",
       "    f\"iai client train --token {IAI_TOKEN} --session {training_session.id} --train-path {data_path}/train_silo0.parquet --test-path {data_path}/test.parquet --batch"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"If I wanted to try multi-client training, where one client has Y, X1 and another client has X2 features, how would I setup the data schema and client train commands?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: You can use the integrate.ai API Sample Notebook to run a session on AWS Batch. The notebook provides an example of how to use the integrate.ai SDK to run a session on AWS Batch. \n",
       "\n",
       "To run the session on AWS Batch, you need to create a job queue and a compute environment. The job queue defines the resources that will be used to run the session, and the compute environment defines the compute resources that will be used to run the session. \n",
       "\n",
       "Once the job queue and compute environment are created, you can submit the session to AWS Batch. The session will be run on the compute resources defined in the compute environment. \n",
       "\n",
       "The following code example shows how to submit a session to AWS Batch: \n",
       "\n",
       "```python\n",
       "# Create a job queue\n",
       "job_queue = batch_client.create_job_queue(job_queue_name='my-job-queue')\n",
       "\n",
       "# Create a compute environment\n",
       "compute_environment = batch_client.create_compute_environment(compute_environment_name='my-compute-environment', job_queue_name=job_queue.job_"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"give me an example of running a session on AWS BATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: This is the constructor for the TemplateModel class. It is used to instantiate the model layers based on the configuration settings. It is called when an instance of the TemplateModel class is created. \n",
       "\n",
       "```python\n",
       "from integrate_ai_sdk.base_class import IaiBaseModule\n",
       "\n",
       "class TemplateModel(IaiBaseModule):\n",
       "    def __init__(self):\n",
       "        \"\"\"\n",
       "        Here you should instantiate your model layers based on the configs.\n",
       "        \"\"\"\n",
       "        super(TemplateModel, self).__init__()\n",
       "\n",
       "    def forward(self):\n",
       "        \"\"\"\n",
       "        The forward path of a model. Can take an input tensor and return a prediction tensor\n",
       "        \"\"\"\n",
       "        pass\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    template_model = TemplateModel()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"can you give the template for building a custom model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: To deploy your model in AWS, you will need to set up an AWS Fargate task runner. This will allow you to run your model in the cloud. \n",
       "\n",
       "1. Create an Amazon ECR repository to store your model images. \n",
       "2. Create an IAM role for your task runner. This role will give your task runner permission to access your data. \n",
       "3. Create a task definition for your model. This will define the resources that your model will use. \n",
       "4. Create a task execution role for your task runner. This will give your task runner permission to access AWS services. \n",
       "5. Create a task runner to run your model. \n",
       "6. Start the task runner. \n",
       "7. Monitor the task runner to ensure that it is running correctly. \n",
       "\n",
       "For more detailed instructions, refer to the AWS Batch and Fargate Manual Setup guide."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"How do I deploy this in AWS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "AI: Integrate.ai supports the following evaluation metrics: accuracy, precision, recall, and F1 score. Additionally, the Federated Loss value for the latest round of model training is reported as the global_model_federated_loss(float) attribute for an instance of SessionMetrics. This is a model level metric reported for each round of training. It is a weighted average loss across different clients, weighted by the number of examples/samples from each silo. See the metrics by machine learning task in the following table:\n",
       "\n",
       "| Classification and Logistic | Regression and Normal | Poisson, Gamma, Inverse Gaussian |\n",
       "| Loss (cross-entropy) | Loss (mean squared error) | Loss (unit deviance) |\n",
       "| ROC-AUC | R2 score | R2 score |\n",
       "| Accuracy | | |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"what evaluation metrics are supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "System: GLM stands for Generalized Linear Model. It is a model class that supports a variety of regression models, such as linear regression, logistic regression, Poisson regression, gamma regression and inverse Gaussian regression models. For example, in Python, you can use the statsmodels library to fit a GLM:\n",
       "\n",
       "```Python\n",
       "import statsmodels.api as sm\n",
       "model = sm.GLM(y, X)\n",
       "results = model.fit()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"what does GLM mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "System: You can train a GLM with integrateai by using the iai_glm model. This model treats GLMs as a special case of single-layer neural nets with particular output activation functions. Here is an example of how to use the iai_glm model in Python:\n",
       "\n",
       "```Python\n",
       "from integrateai.models import iai_glm\n",
       "\n",
       "model = iai_glm(input_dim=10, output_dim=1)\n",
       "model.fit(X_train, y_train)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"how do I train a GLM with integrateai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "System: To create an EDA session, you must first create a session, the same as you would for training a model. In this case, to configure the session, you must specify either the dataset_config, or num_datasets argument. Using a dataset_config file: The dataset_config file is a configuration file that maps the name of one or more datasets to the columns to be pulled. Dataset names and column names are specified as key-value pairs in the file. For each pair, the keys are dataset names that are expected for the EDA analysis. The values are a list of corresponding columns. The list of columns can be specified as column names (strings), column indices (integers), or a blank list to retrieve all columns from that particular dataset. If a dataset name is not included in the configuration file, all columns from that dataset are used by default.\n",
       "\n",
       "To create an EDA session, we specify a `dataset_config` dictionary indicating the columns to explore for each dataset. Here the empty list `[]` means to include all columns. \n",
       "For example:\n",
       "```python\n",
       "eda_session = client.create_eda_session(\n",
       "    name=\"Testing notebook -"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"how to create an EDA session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "System: Sure, here is an example of a PRL data config for a session with two clients: \n",
       "```python\n",
       "prl_data_config = {\n",
       "    \"clients\": {\n",
       "        \"passive_client\": {\"id_columns\": [\"id\"]},\n",
       "        \"active_client\": {\"id_columns\": [\"id\"]},\n",
       "    }\n",
       "}\n",
       "```\n",
       "This data config specifies that the two clients are providing data and that the \"id\" column in any provided datasets will be used to link the two datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"give me an example of data config for PRL sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "System: Currently, the iai_ffnet model supports classification and regression for HFL. Additionally, the integrate.ai HFL Gradient Boosting Methods are also supported."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"which strategies are currently supported for HFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "A VFL session is a type of machine learning session used to link two datasets together. It is used to create a predictive model that can be used to make predictions about future data. In a VFL session, the training data is used to create a model that can be used to make predictions about the test data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(\"what is a VFL session\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"what is pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"how to load parquet files?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"how to load parquet files directly with pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"gimme an example of computing the moving average of all columns in a dataframe\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iai_local_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
